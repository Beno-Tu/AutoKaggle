### **Data Cleaning Plan for "ghouls_goblins_and_ghosts_boo" Competition**

To ensure the integrity and quality of your datasets (`train.csv` and `test.csv`), the following **four** comprehensive tasks have been designed specifically for the Data Cleaning phase. Each task addresses critical aspects of your data, focusing on individual features and ensuring consistency across both datasets.

---

#### **Task 1: Detect and Handle Outliers in Numerical Features Using the IQR Method**

**Objective:**  
Identify and manage outliers in the numerical features (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`) to enhance data quality and model performance.

**Steps:**

1. **Calculate the Interquartile Range (IQR) for Each Numerical Feature:**
   - **Action:** For each numerical feature, compute the first quartile (Q1) and the third quartile (Q3).
   - **Tool:** Utilize Pandas’ `quantile()` function.
   - **Example Parameters:**
     - `Q1 = data['bone_length'].quantile(0.25)`
     - `Q3 = data['bone_length'].quantile(0.75)`

2. **Determine Lower and Upper Boundaries:**
   - **Action:** Calculate the IQR and define the lower and upper thresholds.
   - **Formula:**
     - `IQR = Q3 - Q1`
     - `Lower Bound = Q1 - (1.5 * IQR)`
     - `Upper Bound = Q3 + (1.5 * IQR)`

3. **Identify Outliers:**
   - **Action:** Flag data points that fall outside the lower and upper boundaries.
   - **Tool:** Compare each data point against the calculated thresholds.
   - **Example:**
     - `outliers = data[(data['bone_length'] < Lower Bound) | (data['bone_length'] > Upper Bound)]`

4. **Handle Outliers:**
   - **Decision Criteria:**
     - **Clipping:** Adjust outliers to the nearest boundary value.
     - **Removal:** Exclude outlier data points from the dataset.
   - **Recommendation:**  
     - **Training Data:** Prefer removal if outliers significantly distort the data distribution.
     - **Testing Data:** Prefer clipping to maintain dataset integrity for predictions.
   - **Tool:** Use the `detect_and_handle_outliers_iqr` tool with the `method` parameter set appropriately.
   - **Parameters Example:**
     - `factor = 1.5`
     - `method = 'remove'` for training data.
     - `method = 'clip'` for testing data.

**Features Involved:**  
- `bone_length`
- `rotting_flesh`
- `hair_length`
- `has_soul`

**Expected Output:**  
- **Cleaned Training Data:** Outliers removed, resulting in `cleaned_train.csv`.
- **Cleaned Testing Data:** Outliers clipped, maintaining the original number of samples in `cleaned_test.csv`.

**Constraints:**  
- Ensure that clipping does not distort the feature distribution.
- Avoid removing a substantial portion of the training data to prevent loss of valuable information.
- Maintain consistency in outlier handling across both datasets to ensure model robustness.

---

#### **Task 2: Verify and Ensure Consistency in Categorical Features Across Datasets**

**Objective:**  
Ensure that the categorical feature `color` maintains consistent categories across both training and testing datasets to prevent discrepancies during model training and prediction.

**Steps:**

1. **Identify Unique Categories in Each Dataset:**
   - **Action:** Extract unique values of the `color` feature from both `cleaned_train.csv` and `cleaned_test.csv`.
   - **Tool:** Use Pandas’ `unique()` function.
   - **Example:**
     - `train_colors = cleaned_train['color'].unique()`
     - `test_colors = cleaned_test['color'].unique()`

2. **Compare Categories Between Datasets:**
   - **Action:** Determine if there are any categories present in one dataset but not the other.
   - **Tool:** Utilize set operations.
   - **Example:**
     - `missing_in_test = set(train_colors) - set(test_colors)`
     - `missing_in_train = set(test_colors) - set(train_colors)`

3. **Standardize Categories:**
   - **Action:** If discrepancies are found, standardize the `color` categories.
   - **Methods:**
     - **Mapping Missing Categories:** Assign a default category or map to an existing one.
     - **Encoding Consistent Categories:** Ensure both datasets use the same encoding for categories.
   - **Tool:** Apply Pandas’ `replace()` or `map()` functions.
   - **Example:**
     - `cleaned_train['color'] = cleaned_train['color'].replace('blood', 'red')` (if standardizing to 'red')

4. **Validate Consistency:**
   - **Action:** Re-extract unique categories post-standardization to confirm consistency.
   - **Tool:** Use Pandas’ `unique()` function again.
   - **Example:**
     - `assert set(cleaned_train['color'].unique()) == set(cleaned_test['color'].unique())`

**Features Involved:**  
- `color`

**Expected Output:**  
- **Cleaned Training Data:** Standardized `color` categories in `cleaned_train.csv`.
- **Cleaned Testing Data:** Standardized `color` categories in `cleaned_test.csv`.

**Constraints:**  
- Avoid introducing new categories that do not exist in the original datasets.
- Ensure that the standardization process does not lead to loss of information or misrepresentation of data.
- Maintain the integrity of the original categorical information during standardization.

---

#### **Task 3: Validate and Convert Data Types for All Features**

**Objective:**  
Ensure that each feature in both datasets has the appropriate data type to facilitate accurate analysis and model training.

**Steps:**

1. **Define Expected Data Types:**
   - **Action:** Establish the correct data types for each feature based on the data description.
   - **Mapping:**
     - `id`: Integer (`int64`)
     - `bone_length`: Floating-point (`float64`)
     - `rotting_flesh`: Floating-point (`float64`)
     - `hair_length`: Floating-point (`float64`)
     - `has_soul`: Floating-point (`float64`)
     - `color`: Categorical/Object (`object`)
     - `type`: Categorical/Object (`object`) *(Only in training data)*

2. **Inspect Current Data Types:**
   - **Action:** Check the existing data types of each feature in both `cleaned_train.csv` and `cleaned_test.csv`.
   - **Tool:** Use Pandas’ `dtypes` attribute.
   - **Example:**
     - `cleaned_train.dtypes`
     - `cleaned_test.dtypes`

3. **Identify Mismatches and Convert Data Types:**
   - **Action:** For any feature with an incorrect data type, perform the necessary conversion.
   - **Tool:** Utilize the `convert_data_types` tool with appropriate parameters.
   - **Conversion Examples:**
     - Convert `id` to integer:
       - `convert_data_types(data=cleaned_train, columns=['id'], target_type='int')`
     - Convert `color` to string:
       - `convert_data_types(data=cleaned_test, columns=['color'], target_type='str')`

4. **Handle Conversion Errors:**
   - **Action:** Manage any errors that arise during type conversion, such as non-numeric values in numerical columns.
   - **Method:** Use the `errors='coerce'` parameter to handle invalid parsing gracefully.
   - **Example:**
     - `cleaned_train['bone_length'] = pd.to_numeric(cleaned_train['bone_length'], errors='coerce')`

5. **Verify Successful Conversion:**
   - **Action:** Re-inspect the data types post-conversion to confirm accuracy.
   - **Tool:** Use Pandas’ `dtypes` attribute again.
   - **Example:**
     - `assert cleaned_train['bone_length'].dtype == 'float64'`

**Features Involved:**  
- `id`
- `bone_length`
- `rotting_flesh`
- `hair_length`
- `has_soul`
- `color`
- `type` *(Training Data Only)*

**Expected Output:**  
- **Cleaned Training Data:** Correct data types for all features in `cleaned_train.csv`.
- **Cleaned Testing Data:** Correct data types for all features in `cleaned_test.csv`.

**Constraints:**  
- Ensure that type conversions do not inadvertently introduce NaN values or errors.
- Maintain consistency in data types across both datasets to prevent modeling issues.
- Avoid altering the `type` feature in the testing data, as it does not exist there.

---

#### **Task 4: Confirm Absence of Duplicate Records**

**Objective:**  
Ensure that there are no duplicate entries in either dataset to maintain data integrity and prevent skewed model training.

**Steps:**

1. **Define Duplicate Criteria:**
   - **Action:** Determine the basis for identifying duplicates. Common criteria include identical `id` values or identical sets of feature values.
   - **Criteria Examples:**
     - **Unique Identifier:** Duplicates based on the `id` column.
     - **All Features:** Duplicates based on all feature columns excluding `type` for training data.

2. **Check for Duplicates in Training Data:**
   - **Action:** Identify any duplicate rows in `cleaned_train.csv`.
   - **Tool:** Use Pandas’ `duplicated()` function.
   - **Example:**
     - `duplicates_train = cleaned_train.duplicated(subset=['id'], keep=False)`

3. **Check for Duplicates in Testing Data:**
   - **Action:** Identify any duplicate rows in `cleaned_test.csv`.
   - **Tool:** Use Pandas’ `duplicated()` function.
   - **Example:**
     - `duplicates_test = cleaned_test.duplicated(subset=['id'], keep=False)`

4. **Remove Duplicates if Found:**
   - **Action:** Exclude duplicate rows from the datasets.
   - **Tool:** Utilize the `remove_duplicates` tool with appropriate parameters.
   - **Parameters Example:**
     - `columns=['id']`
     - `keep='first'`
   - **Example:**
     - `cleaned_train = remove_duplicates(data=cleaned_train, columns=['id'], keep='first')`
     - `cleaned_test = remove_duplicates(data=cleaned_test, columns=['id'], keep='first')`

5. **Verify Removal:**
   - **Action:** Ensure that all duplicates have been successfully removed.
   - **Tool:** Re-apply the `duplicated()` function and confirm no duplicates remain.
   - **Example:**
     - `assert not cleaned_train.duplicated(subset=['id']).any()`

**Features Involved:**  
- `id`
- All feature columns (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`, `color`, `type`)

**Expected Output:**  
- **Cleaned Training Data:** No duplicate records in `cleaned_train.csv`.
- **Cleaned Testing Data:** No duplicate records in `cleaned_test.csv`.

**Constraints:**  
- Ensure that the removal of duplicates does not lead to loss of unique information.
- Preserve the first occurrence of a duplicate row as specified by the `keep` parameter.
- Since the previous report indicates no duplicates, this step serves as a validation to confirm data integrity.

---

### **Final Deliverables**

Upon successful completion of the above tasks, you will have:

1. **Cleaned Training Dataset:** `cleaned_train.csv` located at `/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/cleaned_train.csv`
2. **Cleaned Testing Dataset:** `cleaned_test.csv` located at `/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/cleaned_test.csv`

These cleaned datasets will be free from outliers, data type inconsistencies, duplicate records, and will maintain consistent categorical feature representations, thereby ensuring a robust foundation for subsequent phases of your data science competition.

---

**Note:**  
- **Documentation:** Maintain detailed records of all data cleaning actions, including any assumptions or decisions made during the process. This documentation will be invaluable for transparency and future reference.
- **Reproducibility:** Ensure that all data cleaning steps are scripted and reproducible to facilitate collaboration and iterative improvements.