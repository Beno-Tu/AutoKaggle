### TASK 1: Detect and Handle Outliers in Numerical Features Using the IQR Method

#### **THOUGHT PROCESS:**

To effectively handle outliers in the numerical features (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`), we'll employ the Interquartile Range (IQR) method. 

- **For Training Data:**
  - **Method:** Remove outliers to ensure that the model isn't skewed by extreme values.
  - **Reasoning:** Since the training data is small (10 records), we must be cautious not to remove too many data points. However, adhering to the plan, we'll proceed with removal while ensuring minimal data loss.

- **For Testing Data:**
  - **Method:** Clip outliers to the nearest boundary values to maintain the original sample size.
  - **Reasoning:** Removing outliers in the test set could lead to inconsistencies between training and testing distributions. Clipping preserves the dataset size and maintains consistency.

We'll utilize the pre-defined `detect_and_handle_outliers_iqr` tool for this task, ensuring that we pass the correct parameters as specified.

#### **CODE:**

```python
import pandas as pd
import os

# Define file paths
data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
train_file = os.path.join(data_dir, 'train.csv')
test_file = os.path.join(data_dir, 'test.csv')
cleaned_train_file = os.path.join(data_dir, 'cleaned_train.csv')
cleaned_test_file = os.path.join(data_dir, 'cleaned_test.csv')

# Load the datasets
try:
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    print("Datasets loaded successfully.")
except Exception as e:
    print(f"Error loading datasets: {e}")
    raise

# Define numerical columns
numerical_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']

# TASK 1: Detect and Handle Outliers in Numerical Features Using the IQR Method

# Handle outliers in training data by removing them
try:
    train_df = detect_and_handle_outliers_iqr(
        data=train_df.copy(),  # Work on a copy to preserve original data
        columns=numerical_cols,
        factor=1.5,
        method='remove'
    )
    print(f"Training data shape after removing outliers: {train_df.shape}")
except Exception as e:
    print(f"Error handling outliers in training data: {e}")
    raise

# Handle outliers in testing data by clipping them
try:
    test_df = detect_and_handle_outliers_iqr(
        data=test_df.copy(),  # Work on a copy to preserve original data
        columns=numerical_cols,
        factor=1.5,
        method='clip'
    )
    print(f"Testing data shape after clipping outliers: {test_df.shape}")
except Exception as e:
    print(f"Error handling outliers in testing data: {e}")
    raise

# Save the cleaned datasets
try:
    train_df.to_csv(cleaned_train_file, index=False)
    test_df.to_csv(cleaned_test_file, index=False)
    print("TASK 1: Outliers handled and cleaned datasets saved.")
except Exception as e:
    print(f"Error saving cleaned datasets: {e}")
    raise
```

#### **EXPLANATION:**

1. **File Paths Setup:**
   - Utilizes `os.path.join` for better path management.
   - Specifies the directories and filenames for loading and saving data.

2. **Data Loading:**
   - Reads `train.csv` and `test.csv` into pandas DataFrames.
   - Wrapped in a `try-except` block to catch and report any file I/O errors.

3. **Outlier Handling:**
   - **Training Data:**
     - Uses `detect_and_handle_outliers_iqr` with `method='remove'` to eliminate outliers.
     - Operates on a copy of the DataFrame to prevent altering the original data inadvertently.
   - **Testing Data:**
     - Uses the same tool with `method='clip'` to cap outliers at the boundary values.
     - Also works on a copy for consistency and safety.
   - Each operation is enclosed in `try-except` blocks to handle potential errors gracefully.

4. **Saving Cleaned Data:**
   - Writes the cleaned DataFrames to `cleaned_train.csv` and `cleaned_test.csv`.
   - Ensures that data is saved in the specified directory with meaningful filenames.
   - Error handling ensures that issues during the save process are caught and reported.

---

### TASK 2: Verify and Ensure Consistency in Categorical Features Across Datasets

#### **THOUGHT PROCESS:**

Ensuring consistency in categorical features, specifically the `color` column, is crucial for model performance. Inconsistent categories between training and testing datasets can lead to errors during model prediction.

**Approach:**

1. **Identify Unique Categories:**
   - Extract unique categories from both training and testing datasets.

2. **Standardize Categories:**
   - Convert all category values to lowercase to enforce uniformity.
   - This prevents discrepancies caused by case differences (e.g., "Black" vs. "black").

3. **Verify Consistency:**
   - Ensure that both datasets have the same set of categories after standardization.
   - If discrepancies exist, handle them appropriately (e.g., mapping unseen categories to a common label).

Given the small dataset size, we anticipate minimal discrepancies, but it's essential to implement this step to adhere to the data cleaning plan.

#### **CODE:**

```python
# Reload the cleaned datasets
try:
    train_df = pd.read_csv(cleaned_train_file)
    test_df = pd.read_csv(cleaned_test_file)
    print("Cleaned datasets reloaded successfully.")
except Exception as e:
    print(f"Error reloading cleaned datasets: {e}")
    raise

# TASK 2: Verify and Ensure Consistency in Categorical Features Across Datasets

# Identify unique categories in both datasets
train_colors = set(train_df['color'].dropna().unique())
test_colors = set(test_df['color'].dropna().unique())

print(f"Unique colors in training data before standardization: {train_colors}")
print(f"Unique colors in testing data before standardization: {test_colors}")

# Step 1: Standardize categories by converting to lowercase
train_df['color'] = train_df['color'].str.lower()
test_df['color'] = test_df['color'].str.lower()

# Step 2: Re-identify unique categories after standardization
train_colors_std = set(train_df['color'].dropna().unique())
test_colors_std = set(test_df['color'].dropna().unique())

print(f"Unique colors in training data after standardization: {train_colors_std}")
print(f"Unique colors in testing data after standardization: {test_colors_std}")

# Step 3: Verify consistency
if not train_colors_std.issubset(test_colors_std) or not test_colors_std.issubset(train_colors_std):
    # Find discrepancies
    all_colors = train_colors_std.union(test_colors_std)
    print("Discrepancies found in 'color' categories. Handling inconsistencies...")

    # For this example, we'll map any unseen categories in the test set to 'unknown'
    # Identify categories in test not present in train
    unseen_in_test = test_colors_std - train_colors_std
    if unseen_in_test:
        test_df['color'] = test_df['color'].replace(list(unseen_in_test), 'unknown')
        print(f"Replaced unseen colors in test data with 'unknown': {unseen_in_test}")

    # Similarly, handle any categories in train not present in test, if necessary
    unseen_in_train = train_colors_std - test_colors_std
    if unseen_in_train:
        train_df['color'] = train_df['color'].replace(list(unseen_in_train), 'unknown')
        print(f"Replaced unseen colors in training data with 'unknown': {unseen_in_train}")
else:
    print("No discrepancies found in 'color' categories. No additional handling needed.")

# Step 4: Re-validate unique categories after handling discrepancies
train_colors_final = set(train_df['color'].dropna().unique())
test_colors_final = set(test_df['color'].dropna().unique())

print(f"Final unique colors in training data: {train_colors_final}")
print(f"Final unique colors in testing data: {test_colors_final}")

# Save the standardized datasets
try:
    train_df.to_csv(cleaned_train_file, index=False)
    test_df.to_csv(cleaned_test_file, index=False)
    print("TASK 2: Categorical feature 'color' standardized and cleaned datasets saved.")
except Exception as e:
    print(f"Error saving standardized datasets: {e}")
    raise
```

#### **EXPLANATION:**

1. **Reload Cleaned Data:**
   - Ensures that we work with the latest version of the cleaned datasets after outlier handling.

2. **Identifying Unique Categories:**
   - Extracts unique values from the `color` column for both training and testing datasets.
   - Uses `dropna()` to exclude any missing values from the unique category sets.

3. **Standardization Process:**
   - Converts all entries in the `color` column to lowercase using `str.lower()`.
   - This step ensures uniformity, eliminating discrepancies due to case sensitivity.

4. **Verification of Consistency:**
   - Checks whether the standardized categories in the training set are a subset of those in the testing set and vice versa.
   - If discrepancies are found, identifies the specific differing categories.

5. **Handling Discrepancies:**
   - **Mapping Unseen Categories:**
     - Any categories present in one dataset but not the other are replaced with a common label (e.g., `'unknown'`).
     - This approach prevents the introduction of unintended categories and maintains consistency.
   - **Example Handling:**
     - If the test set contains a color not present in the training set, it gets mapped to `'unknown'`.
     - Similarly, if the training set has unique colors absent in the test set, they are also mapped to `'unknown'`.

6. **Final Verification:**
   - Re-extracts unique categories post-handling to ensure consistency across both datasets.

7. **Saving Standardized Data:**
   - Writes the standardized DataFrames back to their respective cleaned CSV files.
   - Ensures that subsequent tasks work with the most up-to-date and consistent data.

---

### TASK 3: Validate and Convert Data Types for All Features

#### **THOUGHT PROCESS:**

Ensuring that all features have the correct data types is vital for accurate data processing and model training. Incorrect data types can lead to unexpected behaviors, errors during analysis, and reduced model performance.

**Approach:**

1. **Define Target Data Types:**
   - **Training Data:**
     - `id`: `int`
     - `bone_length`, `rotting_flesh`, `hair_length`, `has_soul`: `float`
     - `color`: `str`
     - `type`: `str`
   - **Testing Data:**
     - `id`: `int`
     - `bone_length`, `rotting_flesh`, `hair_length`, `has_soul`: `float`
     - `color`: `str`

2. **Conversion Process:**
   - Utilize the pre-defined `convert_data_types` tool to change data types as specified.
   - Handle any conversion errors gracefully, ensuring that invalid entries are coerced to `NaN` where appropriate.

3. **Verification:**
   - After conversion, print the data types of all columns to confirm that changes have been successfully applied.

4. **Constraints:**
   - Do not alter the `type` column in the testing data, as it doesn't exist there.
   - Ensure that `id` columns remain unmodified aside from their data type conversion.

#### **CODE:**

```python
# Reload the cleaned and standardized datasets
try:
    train_df = pd.read_csv(cleaned_train_file)
    test_df = pd.read_csv(cleaned_test_file)
    print("Cleaned and standardized datasets reloaded successfully.")
except Exception as e:
    print(f"Error reloading cleaned datasets: {e}")
    raise

# TASK 3: Validate and Convert Data Types for All Features

# Define data type conversions for training data
type_conversions_train = {
    'id': 'int',
    'bone_length': 'float',
    'rotting_flesh': 'float',
    'hair_length': 'float',
    'has_soul': 'float',
    'color': 'str',
    'type': 'str'
}

# Define data type conversions for testing data
type_conversions_test = {
    'id': 'int',
    'bone_length': 'float',
    'rotting_flesh': 'float',
    'hair_length': 'float',
    'has_soul': 'float',
    'color': 'str'
}

# Convert data types for training data
try:
    # Convert categorical columns first to avoid issues during numeric conversions
    train_df = convert_data_types(
        data=train_df,
        columns=['color', 'type'],
        target_type='str'
    )
    
    # Convert numerical columns
    train_df = convert_data_types(
        data=train_df,
        columns=['bone_length', 'rotting_flesh', 'hair_length', 'has_soul', 'id'],
        target_type='float'  # Temporarily convert 'id' to float to handle NaNs
    )
    
    # Now convert 'id' to int using pandas' Int64 for nullable integers
    train_df['id'] = pd.to_numeric(train_df['id'], errors='coerce').astype('Int64')
    
    print("Training data types after conversion:")
    print(train_df.dtypes)
except Exception as e:
    print(f"Error converting data types for training data: {e}")
    raise

# Convert data types for testing data
try:
    # Convert categorical columns first
    test_df = convert_data_types(
        data=test_df,
        columns=['color'],
        target_type='str'
    )
    
    # Convert numerical columns
    test_df = convert_data_types(
        data=test_df,
        columns=['bone_length', 'rotting_flesh', 'hair_length', 'has_soul', 'id'],
        target_type='float'  # Temporarily convert 'id' to float to handle NaNs
    )
    
    # Now convert 'id' to int using pandas' Int64 for nullable integers
    test_df['id'] = pd.to_numeric(test_df['id'], errors='coerce').astype('Int64')
    
    print("Testing data types after conversion:")
    print(test_df.dtypes)
except Exception as e:
    print(f"Error converting data types for testing data: {e}")
    raise

# Save the datasets with updated data types
try:
    train_df.to_csv(cleaned_train_file, index=False)
    test_df.to_csv(cleaned_test_file, index=False)
    print("TASK 3: Data types validated and converted. Cleaned datasets saved.")
except Exception as e:
    print(f"Error saving datasets after type conversion: {e}")
    raise
```

#### **EXPLANATION:**

1. **Reload Cleaned Data:**
   - Ensures that we're working with the latest version of the data after handling outliers and standardizing categories.

2. **Defining Target Data Types:**
   - Clearly specifies the desired data types for each feature in both training and testing datasets.
   - Differentiates between training and testing data to account for the absence of the `type` column in the test set.

3. **Conversion Process for Training Data:**
   - **Categorical Columns (`color`, `type`):**
     - First converts these to `str` to ensure that string data is correctly formatted.
   - **Numerical Columns (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`, `id`):**
     - Converts to `float` initially to handle any potential `NaN` values resulting from coercion.
   - **`id` Column:**
     - After converting to `float`, it is further converted to `Int64` (nullable integer type in pandas) to maintain integer integrity while allowing for `NaN` if any exist.
   - **Verification:**
     - Prints the data types post-conversion to confirm that all changes have been applied as intended.

4. **Conversion Process for Testing Data:**
   - Mirrors the steps taken for the training data, excluding the `type` column.
   - Ensures that the `color` column is converted to `str` and numerical features to `float`, with `id` subsequently converted to `Int64`.

5. **Saving Converted Data:**
   - Writes the DataFrames with updated data types back to their respective cleaned CSV files.
   - Ensures that subsequent tasks use data with accurate and consistent types, minimizing the risk of errors during analysis or modeling.

6. **Error Handling:**
   - Each conversion step is enclosed in `try-except` blocks to catch and report any issues that arise during the type conversion process.
   - This proactive approach ensures that data integrity is maintained and that any anomalies are promptly addressed.

---

### TASK 4: Confirm Absence of Duplicate Records

#### **THOUGHT PROCESS:**

Duplicate records can introduce bias and distort the analysis or model training process. Ensuring that each entry is unique is crucial for maintaining data integrity.

**Approach:**

1. **Identify Duplicates:**
   - Check for duplicate entries based on the `id` column, which serves as a unique identifier.

2. **Removal Process:**
   - Use the pre-defined `remove_duplicates` tool to eliminate any duplicate records.
   - Preserve the first occurrence of any duplicate to retain as much original information as possible.

3. **Verification:**
   - After removal, confirm that no duplicates remain in both training and testing datasets.
   - Print the shape of the datasets before and after removal to visualize the changes.

4. **Constraints:**
   - Ensure that only duplicates based on the `id` column are removed.
   - Avoid altering or removing unique and valuable information inadvertently.

Given the small size of the datasets, this step should be straightforward, but it's essential to implement it to adhere to the data cleaning plan.

#### **CODE:**

```python
# Reload the datasets with updated data types
try:
    train_df = pd.read_csv(cleaned_train_file)
    test_df = pd.read_csv(cleaned_test_file)
    print("Datasets with updated data types reloaded successfully.")
except Exception as e:
    print(f"Error reloading datasets for duplicate removal: {e}")
    raise

# TASK 4: Confirm Absence of Duplicate Records

# Remove duplicates in training data based on 'id'
try:
    initial_train_shape = train_df.shape
    train_df = remove_duplicates(
        data=train_df.copy(),  # Work on a copy to preserve original data
        columns=['id'],
        keep='first'
    )
    final_train_shape = train_df.shape
    print(f"Training data shape before removing duplicates: {initial_train_shape}")
    print(f"Training data shape after removing duplicates: {final_train_shape}")
except Exception as e:
    print(f"Error removing duplicates from training data: {e}")
    raise

# Remove duplicates in testing data based on 'id'
try:
    initial_test_shape = test_df.shape
    test_df = remove_duplicates(
        data=test_df.copy(),  # Work on a copy to preserve original data
        columns=['id'],
        keep='first'
    )
    final_test_shape = test_df.shape
    print(f"Testing data shape before removing duplicates: {initial_test_shape}")
    print(f"Testing data shape after removing duplicates: {final_test_shape}")
except Exception as e:
    print(f"Error removing duplicates from testing data: {e}")
    raise

# Verify absence of duplicates
train_duplicates = train_df.duplicated(subset=['id']).sum()
test_duplicates = test_df.duplicated(subset=['id']).sum()

print(f"Number of duplicate 'id's in training data after removal: {train_duplicates}")
print(f"Number of duplicate 'id's in testing data after removal: {test_duplicates}")

# Save the final cleaned datasets
try:
    train_df.to_csv(cleaned_train_file, index=False)
    test_df.to_csv(cleaned_test_file, index=False)
    print("TASK 4: Duplicate records removed. Cleaned datasets saved.")
except Exception as e:
    print(f"Error saving datasets after duplicate removal: {e}")
    raise
```

#### **EXPLANATION:**

1. **Reload Datasets:**
   - Ensures that the most recent versions of the cleaned and type-validated datasets are loaded for duplicate checking.

2. **Removing Duplicates in Training Data:**
   - **Initial Shape:**
     - Captures the shape of the training DataFrame before duplicate removal to assess any changes.
   - **Duplicate Removal:**
     - Uses `remove_duplicates` with `columns=['id']` and `keep='first'` to eliminate duplicate entries based on the `id` column.
     - Operates on a copy to prevent unintended modifications to the original DataFrame.
   - **Final Shape:**
     - Captures the shape post-removal to determine how many records were removed, if any.

3. **Removing Duplicates in Testing Data:**
   - Follows the same process as the training data, ensuring that duplicates are handled consistently across both datasets.

4. **Verification of Absence of Duplicates:**
   - Utilizes `duplicated()` to count any remaining duplicate `id` entries in both datasets.
   - Ensures that the duplicate removal process was successful.

5. **Saving Cleaned Data:**
   - Writes the deduplicated DataFrames back to their respective cleaned CSV files.
   - Ensures that the final cleaned datasets are free from duplicate records, maintaining data integrity.

6. **Error Handling:**
   - Each removal and save step is enclosed in `try-except` blocks to catch and report any issues that arise during the process.
   - This approach ensures that the script halts gracefully if unexpected errors occur, preventing the propagation of corrupted data.

---

### SUMMARY OF DATA CLEANING PHASE:

Through the systematic execution of the above tasks, the following outcomes have been achieved:

1. **Outlier Handling:**
   - Training data had outliers removed, ensuring a cleaner dataset for model training.
   - Testing data had outliers clipped, maintaining dataset size and consistency.

2. **Categorical Consistency:**
   - The `color` feature was standardized across both datasets by converting all entries to lowercase.
   - Any discrepancies in categories were addressed by mapping unseen categories to `'unknown'`, ensuring consistency.

3. **Data Type Validation:**
   - All features were assigned appropriate data types, minimizing potential errors in subsequent analysis and modeling.
   - Special attention was given to the `id` column to ensure it remained an integer type without unintended modifications.

4. **Duplicate Removal:**
   - Ensured that both training and testing datasets are free from duplicate records based on the `id` column.
   - Preserved the first occurrence of any duplicates to retain original data integrity.

5. **Saving Cleaned Data:**
   - All cleaned and processed datasets have been saved in the specified directory with clear and meaningful filenames (`cleaned_train.csv` and `cleaned_test.csv`), adhering to the project constraints.

By meticulously following the plan and adhering to the specified constraints, the data cleaning phase has been successfully completed, laying a solid foundation for the subsequent stages of exploratory data analysis, feature engineering, and model building.