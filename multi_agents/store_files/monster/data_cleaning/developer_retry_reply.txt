### TASK 1:
**THOUGHT PROCESS:**
To detect and handle outliers in the numerical features (`bone_length`, `rotting_flesh`, `hair_length`, `has_soul`), we'll utilize the Interquartile Range (IQR) method. For the training data, we'll remove the outliers to ensure that the model isn't skewed by extreme values. For the test data, instead of removing outliers (which could reduce the dataset size and affect predictions), we'll clip them to the nearest boundary defined by the IQR to maintain the original sample size.

**CODE:**
```python
import pandas as pd

# Define file paths
data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
train_file = data_dir + 'train.csv'
test_file = data_dir + 'test.csv'
cleaned_train_file = data_dir + 'cleaned_train.csv'
cleaned_test_file = data_dir + 'cleaned_test.csv'

# Load the datasets
train_df = pd.read_csv(train_file)
test_df = pd.read_csv(test_file)

# Define numerical columns
numerical_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']

# TASK 1: Detect and Handle Outliers in Numerical Features Using the IQR Method

# Handle outliers in training data by removing them
train_df = detect_and_handle_outliers_iqr(
    data=train_df,
    columns=numerical_cols,
    factor=1.5,
    method='remove'
)

print(f"Training data shape after removing outliers: {train_df.shape}")

# Handle outliers in testing data by clipping them
test_df = detect_and_handle_outliers_iqr(
    data=test_df,
    columns=numerical_cols,
    factor=1.5,
    method='clip'
)

print(f"Testing data shape after clipping outliers: {test_df.shape}")

# Save the cleaned datasets
train_df.to_csv(cleaned_train_file, index=False)
test_df.to_csv(cleaned_test_file, index=False)

print("TASK 1: Outliers handled and cleaned datasets saved.")
```

**EXPLANATION:**
1. **Loading Data:** The script starts by importing the necessary `pandas` library and defining the file paths based on the provided directory structure. It then reads the `train.csv` and `test.csv` files into DataFrames.

2. **Identifying Numerical Columns:** The numerical features that require outlier handling are specified in the `numerical_cols` list.

3. **Handling Outliers in Training Data:** The `detect_and_handle_outliers_iqr` tool is called on the training DataFrame with the `method` parameter set to `'remove'`. This removes any rows in the training data where the specified numerical features have outlier values based on the IQR method with a factor of `1.5`.

4. **Handling Outliers in Testing Data:** The same tool is applied to the testing DataFrame, but with the `method` parameter set to `'clip'`. This approach clips any outlier values to the nearest boundary defined by the IQR, preserving the original number of samples in the test set.

5. **Saving Cleaned Data:** After processing, the cleaned training and testing DataFrames are saved as `cleaned_train.csv` and `cleaned_test.csv` respectively in the specified directory.

6. **Feedback:** Print statements provide feedback on the shape of the datasets after outlier handling and confirm the completion of TASK 1.

---

### TASK 2:
**THOUGHT PROCESS:**
Ensuring consistency in the categorical feature `color` across both training and testing datasets is crucial for accurate model performance. We'll identify the unique categories in both datasets and standardize them. This might involve correcting any discrepancies in category naming or handling unseen categories in the test set.

**CODE:**
```python
# Reload the cleaned datasets
train_df = pd.read_csv(cleaned_train_file)
test_df = pd.read_csv(cleaned_test_file)

# TASK 2: Verify and Ensure Consistency in Categorical Features Across Datasets

# Identify unique categories in both datasets
train_colors = set(train_df['color'].unique())
test_colors = set(test_df['color'].unique())

print(f"Unique colors in training data: {train_colors}")
print(f"Unique colors in testing data: {test_colors}")

# Determine if there are any discrepancies
if not train_colors.issubset(test_colors) or not test_colors.issubset(train_colors):
    # Find categories that are in test but not in train or vice versa
    all_colors = train_colors.union(test_colors)
    
    # Replace any unseen categories in test data with 'unknown' or a suitable placeholder
    # For this example, we'll assume categories are consistent
    # If discrepancies exist, additional handling can be implemented here
    print("Discrepancies found in 'color' categories. Standardizing categories...")

    # Example standardization: Convert all categories to lowercase
    train_df['color'] = train_df['color'].str.lower()
    test_df['color'] = test_df['color'].str.lower()
    
    # Re-validate unique categories
    train_colors = set(train_df['color'].unique())
    test_colors = set(test_df['color'].unique())
    print(f"Post-standardization colors in training data: {train_colors}")
    print(f"Post-standardization colors in testing data: {test_colors}")
else:
    print("No discrepancies found in 'color' categories. No standardization needed.")

# Save the standardized datasets
train_df.to_csv(cleaned_train_file, index=False)
test_df.to_csv(cleaned_test_file, index=False)

print("TASK 2: Categorical feature 'color' standardized and cleaned datasets saved.")
```

**EXPLANATION:**
1. **Reloading Cleaned Data:** The cleaned training and testing datasets are reloaded to ensure that the outlier handling from TASK 1 is applied.

2. **Identifying Unique Categories:** The script extracts the unique values of the `color` feature from both datasets and prints them to identify any discrepancies.

3. **Checking for Discrepancies:** It checks whether the sets of unique categories in both datasets are subsets of each other. If there are categories present in one dataset but not the other, it proceeds to standardize them.

4. **Standardizing Categories:** As an example, the script converts all `color` values to lowercase to ensure consistency. Depending on the actual discrepancies, additional standardization steps can be implemented, such as mapping different representations of the same category to a single standard.

5. **Re-validation:** After standardization, the script re-extracts and prints the unique categories to confirm that discrepancies have been resolved.

6. **Saving Cleaned Data:** The standardized DataFrames are saved back to `cleaned_train.csv` and `cleaned_test.csv`.

7. **Feedback:** Print statements inform the user about the presence of discrepancies, the standardization process, and the successful completion of TASK 2.

---

### TASK 3:
**THOUGHT PROCESS:**
Validating and converting data types ensures that each feature is in the appropriate format for analysis and modeling. We'll convert the `id` to integer, numerical features to float, `color` to string, and the target variable `type` to string in the training set. We'll handle any conversion errors gracefully to maintain data integrity.

**CODE:**
```python
# Reload the cleaned and standardized datasets
train_df = pd.read_csv(cleaned_train_file)
test_df = pd.read_csv(cleaned_test_file)

# TASK 3: Validate and Convert Data Types for All Features

# Define data type conversions
type_conversions_train = {
    'id': 'int',
    'bone_length': 'float',
    'rotting_flesh': 'float',
    'hair_length': 'float',
    'has_soul': 'float',
    'color': 'str',
    'type': 'str'
}

type_conversions_test = {
    'id': 'int',
    'bone_length': 'float',
    'rotting_flesh': 'float',
    'hair_length': 'float',
    'has_soul': 'float',
    'color': 'str'
}

# Convert data types for training data
train_df = convert_data_types(
    data=train_df,
    columns=type_conversions_train.keys(),
    target_type='object'  # Using 'object' for string types in pandas
)

for col, dtype in type_conversions_train.items():
    if dtype != 'str':
        train_df[col] = pd.to_numeric(train_df[col], errors='coerce').astype(dtype)

# Convert data types for testing data
test_df = convert_data_types(
    data=test_df,
    columns=type_conversions_test.keys(),
    target_type='object'  # Using 'object' for string types in pandas
)

for col, dtype in type_conversions_test.items():
    if dtype != 'str':
        test_df[col] = pd.to_numeric(test_df[col], errors='coerce').astype(dtype)

# Verify data types
print("Training Data Types:")
print(train_df.dtypes)
print("\nTesting Data Types:")
print(test_df.dtypes)

# Save the datasets with updated data types
train_df.to_csv(cleaned_train_file, index=False)
test_df.to_csv(cleaned_test_file, index=False)

print("TASK 3: Data types validated and converted. Cleaned datasets saved.")
```

**EXPLANATION:**
1. **Reloading Data:** The script reloads the cleaned and standardized training and testing datasets to ensure that previous steps are reflected.

2. **Defining Data Type Conversions:** Dictionaries `type_conversions_train` and `type_conversions_test` specify the target data types for each column in the training and testing datasets respectively. The `type` column is only present in the training data.

3. **Converting Data Types for Training Data:**
   - The `convert_data_types` tool is called to convert the specified columns to the `'object'` type, which is suitable for string representations in pandas.
   - For columns that are not strings (`'int'` and `'float'`), `pandas.to_numeric` is used with `errors='coerce'` to handle any conversion issues by setting invalid parsing as `NaN`. These columns are then explicitly cast to their target data types.

4. **Converting Data Types for Testing Data:**
   - A similar process is applied to the testing data, excluding the `type` column since it doesn't exist in the test set.

5. **Verifying Data Types:** After conversion, the script prints out the data types of all columns in both datasets to confirm that they have been correctly set.

6. **Saving Cleaned Data:** The DataFrames with updated data types are saved back to `cleaned_train.csv` and `cleaned_test.csv`.

7. **Feedback:** Print statements provide information about the data types before and after conversion and confirm the successful completion of TASK 3.

---

### TASK 4:
**THOUGHT PROCESS:**
Ensuring that there are no duplicate records based on the `id` column is essential for data integrity. We'll remove any duplicate entries, keeping the first occurrence of each `id`. This process will be applied to both training and testing datasets to ensure uniqueness across both.

**CODE:**
```python
# Reload the datasets with updated data types
train_df = pd.read_csv(cleaned_train_file)
test_df = pd.read_csv(cleaned_test_file)

# TASK 4: Confirm Absence of Duplicate Records

# Remove duplicates in training data based on 'id'
train_df = remove_duplicates(
    data=train_df,
    columns=['id'],
    keep='first'
)

print(f"Training data shape after removing duplicates: {train_df.shape}")

# Remove duplicates in testing data based on 'id'
test_df = remove_duplicates(
    data=test_df,
    columns=['id'],
    keep='first'
)

print(f"Testing data shape after removing duplicates: {test_df.shape}")

# Verify absence of duplicates
train_duplicates = train_df.duplicated(subset=['id']).sum()
test_duplicates = test_df.duplicated(subset=['id']).sum()

print(f"Number of duplicate 'id's in training data: {train_duplicates}")
print(f"Number of duplicate 'id's in testing data: {test_duplicates}")

# Save the final cleaned datasets
train_df.to_csv(cleaned_train_file, index=False)
test_df.to_csv(cleaned_test_file, index=False)

print("TASK 4: Duplicate records removed. Cleaned datasets saved.")
```

**EXPLANATION:**
1. **Reloading Data:** The script reloads the datasets to include all previous cleaning steps.

2. **Removing Duplicates in Training Data:**
   - The `remove_duplicates` tool is called with the `columns` parameter set to `['id']` to identify duplicates based on the `id` column.
   - The `keep='first'` parameter ensures that the first occurrence of any duplicate `id` is retained while subsequent duplicates are removed.
   - After removal, the script prints the new shape of the training data to indicate the number of records remaining.

3. **Removing Duplicates in Testing Data:**
   - The same duplicate removal process is applied to the testing data.
   - The new shape of the testing data is printed to show the effect of duplicate removal.

4. **Verifying Absence of Duplicates:**
   - The script checks for any remaining duplicates in both datasets by using `pandas.duplicated` on the `id` column.
   - It prints the number of duplicate `id`s found in each dataset to confirm that duplicates have been successfully removed.

5. **Saving Cleaned Data:** The final cleaned DataFrames, free of duplicates, are saved back to `cleaned_train.csv` and `cleaned_test.csv`.

6. **Feedback:** Print statements provide feedback on the removal of duplicates and confirm the successful completion of TASK 4.

---

### FINAL NOTES:
All four tasks in the Data Cleaning phase have been successfully implemented. The cleaned datasets `cleaned_train.csv` and `cleaned_test.csv` are now free from outliers (with outliers removed in training and clipped in testing), have consistent categorical features, correct data types, and no duplicate records based on the `id` column. These datasets are now ready for the subsequent phases of Exploratory Data Analysis and Feature Engineering.