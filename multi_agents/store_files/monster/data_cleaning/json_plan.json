[
    {
        "task": "Detect and Handle Outliers in Numerical Features Using the IQR Method",
        "tools, involved features and correct parameters": [
            "Tool: detect_and_handle_outliers_iqr",
            "Parameters for Training Data:",
            "- columns: ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']",
            "- factor: 1.5",
            "- method: 'remove'",
            "Parameters for Testing Data:",
            "- columns: ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']",
            "- factor: 1.5",
            "- method: 'clip'",
            "Features Involved: bone_length, rotting_flesh, hair_length, has_soul"
        ],
        "expected output or impact on data": [
            "Training Data: Outliers are removed, resulting in a cleaned cleaned_train.csv.",
            "Testing Data: Outliers are clipped to the nearest boundary values, maintaining the original sample size in cleaned_test.csv."
        ],
        "constraints": [
            "Clipping should not distort the overall feature distribution.",
            "Avoid removing a large portion of the training data to preserve valuable information.",
            "Ensure consistent outlier handling between training and testing datasets to maintain model robustness."
        ]
    },
    {
        "task": "Verify and Ensure Consistency in Categorical Features Across Datasets",
        "tools, involved features and correct parameters": [
            "Tools:",
            "- Pandas\u2019 unique() and set operations for identifying unique categories.",
            "- Pandas\u2019 replace() or map() for standardizing categories.",
            "Features Involved: color"
        ],
        "expected output or impact on data": [
            "Both Datasets: color categories are standardized and consistent across cleaned_train.csv and cleaned_test.csv."
        ],
        "constraints": [
            "Do not introduce new categories that were not originally present.",
            "Ensure that standardization does not lead to information loss or misrepresentation of the original data.",
            "Maintain the integrity of categorical information during the standardization process."
        ]
    },
    {
        "task": "Validate and Convert Data Types for All Features",
        "tools, involved features and correct parameters": [
            "Tool: convert_data_types",
            "Parameters:",
            "- id: int",
            "- bone_length, rotting_flesh, hair_length, has_soul: float",
            "- color: str",
            "- type (Training Data Only): str",
            "Additional Actions:",
            "- Use Pandas\u2019 to_numeric() with errors='coerce' for handling conversion errors.",
            "Features Involved: id, bone_length, rotting_flesh, hair_length, has_soul, color, type (training only)"
        ],
        "expected output or impact on data": [
            "Both Datasets: All features have the correct and consistent data types in cleaned_train.csv and cleaned_test.csv.",
            "Data Integrity: Minimizes errors during subsequent analysis and model training by ensuring appropriate data types."
        ],
        "constraints": [
            "Prevent inadvertent introduction of NaN values or errors during type conversions.",
            "Maintain consistent data types across both datasets to avoid modeling issues.",
            "Do not alter the type feature in the testing data, as it does not exist there."
        ]
    },
    {
        "task": "Confirm Absence of Duplicate Records",
        "tools, involved features and correct parameters": [
            "Tool: remove_duplicates",
            "Parameters:",
            "- columns: ['id']",
            "- keep: 'first'",
            "Features Involved: id, bone_length, rotting_flesh, hair_length, has_soul, color, type"
        ],
        "expected output or impact on data": [
            "Both Datasets: All duplicate records are removed, ensuring cleaned_train.csv and cleaned_test.csv contain only unique entries."
        ],
        "constraints": [
            "Preserve the first occurrence of any duplicate row as specified by the keep parameter.",
            "Ensure that removing duplicates does not result in the loss of unique and valuable information.",
            "Validate that no duplicates exist after the removal process, as indicated by previous reports."
        ]
    }
]