
import pandas as pd
import os
import numpy as np
import logging

# Configure logging
logging.basicConfig(
    filename='feature_engineering.log',
    filemode='a',
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

def load_data(data_dir, train_file, test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, train_file))
        test_df = pd.read_csv(os.path.join(data_dir, test_file))
        logging.info("Cleaned datasets loaded successfully.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"File not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading data: {e}")
        raise

def create_derived_features(df):
    try:
        df['hair_length_has_soul'] = df['hair_length'] * df['has_soul']
        df['bone_length_rotting_flesh'] = df['bone_length'] * df['rotting_flesh']
        df['bone_length_squared'] = df['bone_length'] ** 2
        df['rotting_flesh_squared'] = df['rotting_flesh'] ** 2
        epsilon = 1e-5  # To prevent division by zero
        df['bone_to_flesh_ratio'] = df['bone_length'] / (df['rotting_flesh'] + epsilon)
        df['soul_to_hair_ratio'] = df['has_soul'] / (df['hair_length'] + epsilon)
        logging.info("Derived features created successfully.")
        return df
    except KeyError as e:
        logging.error(f"Missing column during feature creation: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature creation: {e}")
        raise

def handle_infinite_nan(df):
    try:
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        logging.info("Infinite and NaN values handled successfully.")
        return df
    except Exception as e:
        logging.error(f"Error handling infinite or NaN values: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    cleaned_train_file = 'cleaned_train.csv'
    cleaned_test_file = 'cleaned_test.csv'
    
    # Load data
    train_df, test_df = load_data(data_dir, cleaned_train_file, cleaned_test_file)
    
    # Create derived features
    train_df = create_derived_features(train_df)
    test_df = create_derived_features(test_df)
    
    # Handle infinite and NaN values
    train_df = handle_infinite_nan(train_df)
    test_df = handle_infinite_nan(test_df)
    
    # Save the intermediate datasets
    processed_train_file = 'processed_train.csv'
    processed_test_file = 'processed_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, processed_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, processed_test_file), index=False)
        logging.info("Derived features added and intermediate datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving processed datasets: {e}")
        raise

if __name__ == "__main__":
    main()


import pandas as pd
import os
import logging
from sklearn.preprocessing import OneHotEncoder

def load_processed_data(data_dir, processed_train_file, processed_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, processed_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, processed_test_file))
        logging.info("Processed datasets loaded successfully for encoding.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Processed file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing processed CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading processed data: {e}")
        raise

def encode_categorical(train_df, test_df, categorical_cols):
    try:
        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)
        ohe.fit(train_df[categorical_cols])
        
        # Transform training data
        train_ohe = pd.DataFrame(
            ohe.transform(train_df[categorical_cols]),
            columns=ohe.get_feature_names_out(categorical_cols),
            index=train_df.index
        )
        
        # Transform testing data
        test_ohe = pd.DataFrame(
            ohe.transform(test_df[categorical_cols]),
            columns=ohe.get_feature_names_out(categorical_cols),
            index=test_df.index
        )
        
        # Drop original categorical columns
        train_df.drop(columns=categorical_cols, inplace=True)
        test_df.drop(columns=categorical_cols, inplace=True)
        
        # Concatenate One-Hot Encoded columns
        train_df = pd.concat([train_df, train_ohe], axis=1)
        test_df = pd.concat([test_df, test_ohe], axis=1)
        
        logging.info("Categorical variables encoded successfully using One-Hot Encoding.")
        return train_df, test_df
    except KeyError as e:
        logging.error(f"Categorical column missing during encoding: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during categorical encoding: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    processed_train_file = 'processed_train.csv'
    processed_test_file = 'processed_test.csv'
    
    # Load processed data
    train_df, test_df = load_processed_data(data_dir, processed_train_file, processed_test_file)
    
    # Define categorical columns
    categorical_cols = ['color']
    
    # Encode categorical variables
    train_df, test_df = encode_categorical(train_df, test_df, categorical_cols)
    
    # Save the datasets after encoding
    encoded_train_file = 'encoded_train.csv'
    encoded_test_file = 'encoded_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, encoded_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, encoded_test_file), index=False)
        logging.info("Categorical variables encoded and datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving encoded datasets: {e}")
        raise

if __name__ == "__main__":
    main()


import pandas as pd
import os
import logging
from sklearn.preprocessing import StandardScaler

def load_encoded_data(data_dir, encoded_train_file, encoded_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, encoded_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, encoded_test_file))
        logging.info("Encoded datasets loaded successfully for scaling.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Encoded file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing encoded CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading encoded data: {e}")
        raise

def scale_features(train_df, test_df, numerical_cols):
    try:
        scaler = StandardScaler()
        scaler.fit(train_df[numerical_cols])
        
        # Transform training data
        train_scaled = scaler.transform(train_df[numerical_cols])
        train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_cols, index=train_df.index)
        train_df[numerical_cols] = train_scaled_df
        
        # Transform testing data
        test_scaled = scaler.transform(test_df[numerical_cols])
        test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_cols, index=test_df.index)
        test_df[numerical_cols] = test_scaled_df
        
        logging.info("Numerical features scaled successfully using StandardScaler.")
        return train_df, test_df
    except KeyError as e:
        logging.error(f"Numerical column missing during scaling: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature scaling: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    encoded_train_file = 'encoded_train.csv'
    encoded_test_file = 'encoded_test.csv'
    
    # Load encoded data
    train_df, test_df = load_encoded_data(data_dir, encoded_train_file, encoded_test_file)
    
    # Define numerical columns
    # Exclude 'id' and 'type' from scaling
    numerical_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul',
                      'hair_length_has_soul', 'bone_length_rotting_flesh',
                      'bone_length_squared', 'rotting_flesh_squared',
                      'bone_to_flesh_ratio', 'soul_to_hair_ratio']
    
    # Scale numerical features
    train_df, test_df = scale_features(train_df, test_df, numerical_cols)
    
    # Save the scaled datasets
    scaled_train_file = 'scaled_train.csv'
    scaled_test_file = 'scaled_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, scaled_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, scaled_test_file), index=False)
        logging.info("Numerical features scaled and datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving scaled datasets: {e}")
        raise

if __name__ == "__main__":
    main()


import pandas as pd
import os
import logging
from sklearn.ensemble import RandomForestClassifier

def load_scaled_data(data_dir, scaled_train_file, scaled_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, scaled_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, scaled_test_file))
        logging.info("Scaled datasets loaded successfully for feature selection.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Scaled file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing scaled CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading scaled data: {e}")
        raise

def select_top_features(train_df, target_column, top_n=10):
    try:
        X = train_df.drop(columns=['id', target_column])
        y = train_df[target_column]
        
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        feature_importances = pd.Series(rf.feature_importances_, index=X.columns)
        top_features = feature_importances.sort_values(ascending=False).head(top_n).index.tolist()
        
        logging.info(f"Top {top_n} features selected based on feature importances.")
        logging.debug(f"Feature importances: {feature_importances}")
        return top_features
    except KeyError as e:
        logging.error(f"Target or feature column missing during feature selection: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature selection: {e}")
        raise

def filter_features(df, top_features, include_target=True):
    try:
        columns = ['id'] + top_features
        if include_target:
            columns.append('type')
        return df[columns]
    except KeyError as e:
        logging.error(f"One or more top features missing in the dataset: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature filtering: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    scaled_train_file = 'scaled_train.csv'
    scaled_test_file = 'scaled_test.csv'
    
    # Load scaled data
    train_df, test_df = load_scaled_data(data_dir, scaled_train_file, scaled_test_file)
    
    # Define target column
    target_column = 'type'
    
    # Select top features
    top_features = select_top_features(train_df, target_column, top_n=10)
    
    # Filter training data to include only top features
    train_selected = filter_features(train_df, top_features, include_target=True)
    
    # Filter testing data to include only top features (exclude target)
    test_selected = filter_features(test_df, top_features, include_target=False)
    
    # Save the final processed datasets
    final_processed_train = 'processed_train.csv'
    final_processed_test = 'processed_test.csv'
    
    try:
        train_selected.to_csv(os.path.join(data_dir, final_processed_train), index=False)
        test_selected.to_csv(os.path.join(data_dir, final_processed_test), index=False)
        logging.info("Feature selection completed and final processed datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving final processed datasets: {e}")
        raise

if __name__ == "__main__":
    main()
