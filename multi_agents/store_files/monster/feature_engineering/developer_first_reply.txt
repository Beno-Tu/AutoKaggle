### Subtask 2: Develop a New Solution Based on the Previous Experience and Suggestions

#### **TASK 1: Create Interaction and Derived Features**

**THOUGHT PROCESS:**
To enhance the dataset's feature space, we'll create interaction and derived features. This includes interaction terms between numerical features, polynomial features to capture non-linear relationships, and ratio features to normalize relationships between different attributes. We'll ensure that no multicollinearity is introduced and handle any potential division by zero by adding a small epsilon.

**CODE:**
```python
import pandas as pd
import os
import numpy as np
import logging

# Configure logging
logging.basicConfig(
    filename='feature_engineering.log',
    filemode='a',
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

def load_data(data_dir, train_file, test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, train_file))
        test_df = pd.read_csv(os.path.join(data_dir, test_file))
        logging.info("Cleaned datasets loaded successfully.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"File not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading data: {e}")
        raise

def create_derived_features(df):
    try:
        df['hair_length_has_soul'] = df['hair_length'] * df['has_soul']
        df['bone_length_rotting_flesh'] = df['bone_length'] * df['rotting_flesh']
        df['bone_length_squared'] = df['bone_length'] ** 2
        df['rotting_flesh_squared'] = df['rotting_flesh'] ** 2
        epsilon = 1e-5  # To prevent division by zero
        df['bone_to_flesh_ratio'] = df['bone_length'] / (df['rotting_flesh'] + epsilon)
        df['soul_to_hair_ratio'] = df['has_soul'] / (df['hair_length'] + epsilon)
        logging.info("Derived features created successfully.")
        return df
    except KeyError as e:
        logging.error(f"Missing column during feature creation: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature creation: {e}")
        raise

def handle_infinite_nan(df):
    try:
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        logging.info("Infinite and NaN values handled successfully.")
        return df
    except Exception as e:
        logging.error(f"Error handling infinite or NaN values: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    cleaned_train_file = 'cleaned_train.csv'
    cleaned_test_file = 'cleaned_test.csv'
    
    # Load data
    train_df, test_df = load_data(data_dir, cleaned_train_file, cleaned_test_file)
    
    # Create derived features
    train_df = create_derived_features(train_df)
    test_df = create_derived_features(test_df)
    
    # Handle infinite and NaN values
    train_df = handle_infinite_nan(train_df)
    test_df = handle_infinite_nan(test_df)
    
    # Save the intermediate datasets
    processed_train_file = 'processed_train.csv'
    processed_test_file = 'processed_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, processed_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, processed_test_file), index=False)
        logging.info("Derived features added and intermediate datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving processed datasets: {e}")
        raise

if __name__ == "__main__":
    main()
```

**EXPLANATION:**
- **Logging:** Configured a logging mechanism to track the progress and capture any errors during execution.
- **Data Loading:** Defined a `load_data` function to read the cleaned training and testing datasets, ensuring error handling for missing files or parsing issues.
- **Feature Creation:** The `create_derived_features` function generates interaction terms (`hair_length_has_soul`, `bone_length_rotting_flesh`), polynomial features (`bone_length_squared`, `rotting_flesh_squared`), and ratio features (`bone_to_flesh_ratio`, `soul_to_hair_ratio`). An epsilon is added to denominators to prevent division by zero.
- **Handling Infinite and NaN Values:** The `handle_infinite_nan` function replaces infinite values with NaN and fills missing numeric values with the median of each column to maintain data integrity.
- **Main Execution:** The `main` function orchestrates the loading of data, creation of derived features, handling of invalid values, and saves the intermediate processed datasets. Errors during saving are logged and raised for visibility.

---

#### **TASK 2: Encode Categorical Variables**

**THOUGHT PROCESS:**
To prepare the categorical `color` feature for model ingestion, we'll apply One-Hot Encoding. This method converts categorical variables into a format that can be provided to machine learning algorithms to do a better job in prediction. We'll ensure that the encoding is consistent between training and testing datasets and handle any unseen categories gracefully.

**CODE:**
```python
import pandas as pd
import os
import logging
from sklearn.preprocessing import OneHotEncoder

def load_processed_data(data_dir, processed_train_file, processed_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, processed_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, processed_test_file))
        logging.info("Processed datasets loaded successfully for encoding.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Processed file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing processed CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading processed data: {e}")
        raise

def encode_categorical(train_df, test_df, categorical_cols):
    try:
        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)
        ohe.fit(train_df[categorical_cols])
        
        # Transform training data
        train_ohe = pd.DataFrame(
            ohe.transform(train_df[categorical_cols]),
            columns=ohe.get_feature_names_out(categorical_cols),
            index=train_df.index
        )
        
        # Transform testing data
        test_ohe = pd.DataFrame(
            ohe.transform(test_df[categorical_cols]),
            columns=ohe.get_feature_names_out(categorical_cols),
            index=test_df.index
        )
        
        # Drop original categorical columns
        train_df.drop(columns=categorical_cols, inplace=True)
        test_df.drop(columns=categorical_cols, inplace=True)
        
        # Concatenate One-Hot Encoded columns
        train_df = pd.concat([train_df, train_ohe], axis=1)
        test_df = pd.concat([test_df, test_ohe], axis=1)
        
        logging.info("Categorical variables encoded successfully using One-Hot Encoding.")
        return train_df, test_df
    except KeyError as e:
        logging.error(f"Categorical column missing during encoding: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during categorical encoding: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    processed_train_file = 'processed_train.csv'
    processed_test_file = 'processed_test.csv'
    
    # Load processed data
    train_df, test_df = load_processed_data(data_dir, processed_train_file, processed_test_file)
    
    # Define categorical columns
    categorical_cols = ['color']
    
    # Encode categorical variables
    train_df, test_df = encode_categorical(train_df, test_df, categorical_cols)
    
    # Save the datasets after encoding
    encoded_train_file = 'encoded_train.csv'
    encoded_test_file = 'encoded_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, encoded_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, encoded_test_file), index=False)
        logging.info("Categorical variables encoded and datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving encoded datasets: {e}")
        raise

if __name__ == "__main__":
    main()
```

**EXPLANATION:**
- **Data Loading:** The `load_processed_data` function reads the intermediate processed datasets (`processed_train.csv` and `processed_test.csv`).
- **One-Hot Encoding:** The `encode_categorical` function applies One-Hot Encoding to the `color` column. It ensures that the encoder is fitted on the training data and transforms both training and testing datasets consistently, ignoring any unknown categories in the test set.
- **Saving Encoded Data:** After encoding, the datasets are saved as `encoded_train.csv` and `encoded_test.csv`. This separation ensures clarity in the processing pipeline.
- **Error Handling:** Comprehensive error handling ensures that missing columns or unexpected issues during encoding are logged and raised appropriately.

---

#### **TASK 3: Normalize and Scale Numerical Features**

**THOUGHT PROCESS:**
Scaling numerical features is crucial for algorithms that are sensitive to the scale of data, such as Support Vector Machines or Neural Networks. We'll apply Standardization (Z-score Scaling) to transform the features to have a mean of 0 and a standard deviation of 1. The scaler will be fitted on the training data and applied to both training and testing datasets to maintain consistency.

**CODE:**
```python
import pandas as pd
import os
import logging
from sklearn.preprocessing import StandardScaler

def load_encoded_data(data_dir, encoded_train_file, encoded_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, encoded_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, encoded_test_file))
        logging.info("Encoded datasets loaded successfully for scaling.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Encoded file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing encoded CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading encoded data: {e}")
        raise

def scale_features(train_df, test_df, numerical_cols):
    try:
        scaler = StandardScaler()
        scaler.fit(train_df[numerical_cols])
        
        # Transform training data
        train_scaled = scaler.transform(train_df[numerical_cols])
        train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_cols, index=train_df.index)
        train_df[numerical_cols] = train_scaled_df
        
        # Transform testing data
        test_scaled = scaler.transform(test_df[numerical_cols])
        test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_cols, index=test_df.index)
        test_df[numerical_cols] = test_scaled_df
        
        logging.info("Numerical features scaled successfully using StandardScaler.")
        return train_df, test_df
    except KeyError as e:
        logging.error(f"Numerical column missing during scaling: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature scaling: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    encoded_train_file = 'encoded_train.csv'
    encoded_test_file = 'encoded_test.csv'
    
    # Load encoded data
    train_df, test_df = load_encoded_data(data_dir, encoded_train_file, encoded_test_file)
    
    # Define numerical columns
    # Exclude 'id' and 'type' from scaling
    numerical_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul',
                      'hair_length_has_soul', 'bone_length_rotting_flesh',
                      'bone_length_squared', 'rotting_flesh_squared',
                      'bone_to_flesh_ratio', 'soul_to_hair_ratio']
    
    # Scale numerical features
    train_df, test_df = scale_features(train_df, test_df, numerical_cols)
    
    # Save the scaled datasets
    scaled_train_file = 'scaled_train.csv'
    scaled_test_file = 'scaled_test.csv'
    
    try:
        train_df.to_csv(os.path.join(data_dir, scaled_train_file), index=False)
        test_df.to_csv(os.path.join(data_dir, scaled_test_file), index=False)
        logging.info("Numerical features scaled and datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving scaled datasets: {e}")
        raise

if __name__ == "__main__":
    main()
```

**EXPLANATION:**
- **Data Loading:** The `load_encoded_data` function reads the One-Hot Encoded datasets (`encoded_train.csv` and `encoded_test.csv`).
- **Standard Scaling:** The `scale_features` function applies `StandardScaler` to the numerical features, ensuring that scaling parameters are derived from the training data and consistently applied to both datasets.
- **Numerical Columns:** Defined a list of numerical columns, excluding identifiers like `id` and the target variable `type`, to avoid unintended scaling.
- **Saving Scaled Data:** The scaled datasets are saved as `scaled_train.csv` and `scaled_test.csv` for subsequent processing.
- **Error Handling:** Ensures that any missing columns or unexpected issues during scaling are captured and logged.

---

#### **TASK 4: Feature Selection and Dimensionality Reduction**

**THOUGHT PROCESS:**
To improve model performance and reduce computational complexity, we'll perform feature selection based on feature importance derived from a `RandomForestClassifier`. We'll select the top 10 features that contribute the most to predicting the target variable. This step helps in eliminating redundant or less significant features, potentially enhancing the model's generalization capability.

**CODE:**
```python
import pandas as pd
import os
import logging
from sklearn.ensemble import RandomForestClassifier

def load_scaled_data(data_dir, scaled_train_file, scaled_test_file):
    try:
        train_df = pd.read_csv(os.path.join(data_dir, scaled_train_file))
        test_df = pd.read_csv(os.path.join(data_dir, scaled_test_file))
        logging.info("Scaled datasets loaded successfully for feature selection.")
        return train_df.copy(), test_df.copy()
    except FileNotFoundError as e:
        logging.error(f"Scaled file not found: {e}")
        raise
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing scaled CSV files: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error loading scaled data: {e}")
        raise

def select_top_features(train_df, target_column, top_n=10):
    try:
        X = train_df.drop(columns=['id', target_column])
        y = train_df[target_column]
        
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        feature_importances = pd.Series(rf.feature_importances_, index=X.columns)
        top_features = feature_importances.sort_values(ascending=False).head(top_n).index.tolist()
        
        logging.info(f"Top {top_n} features selected based on feature importances.")
        logging.debug(f"Feature importances: {feature_importances}")
        return top_features
    except KeyError as e:
        logging.error(f"Target or feature column missing during feature selection: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature selection: {e}")
        raise

def filter_features(df, top_features, include_target=True):
    try:
        columns = ['id'] + top_features
        if include_target:
            columns.append('type')
        return df[columns]
    except KeyError as e:
        logging.error(f"One or more top features missing in the dataset: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error during feature filtering: {e}")
        raise

def main():
    # Define file paths
    data_dir = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/ghouls_goblins_and_ghosts_boo/'
    scaled_train_file = 'scaled_train.csv'
    scaled_test_file = 'scaled_test.csv'
    
    # Load scaled data
    train_df, test_df = load_scaled_data(data_dir, scaled_train_file, scaled_test_file)
    
    # Define target column
    target_column = 'type'
    
    # Select top features
    top_features = select_top_features(train_df, target_column, top_n=10)
    
    # Filter training data to include only top features
    train_selected = filter_features(train_df, top_features, include_target=True)
    
    # Filter testing data to include only top features (exclude target)
    test_selected = filter_features(test_df, top_features, include_target=False)
    
    # Save the final processed datasets
    final_processed_train = 'processed_train.csv'
    final_processed_test = 'processed_test.csv'
    
    try:
        train_selected.to_csv(os.path.join(data_dir, final_processed_train), index=False)
        test_selected.to_csv(os.path.join(data_dir, final_processed_test), index=False)
        logging.info("Feature selection completed and final processed datasets saved successfully.")
    except Exception as e:
        logging.error(f"Error saving final processed datasets: {e}")
        raise

if __name__ == "__main__":
    main()
```

**EXPLANATION:**
- **Data Loading:** The `load_scaled_data` function reads the scaled datasets (`scaled_train.csv` and `scaled_test.csv`).
- **Feature Selection:** The `select_top_features` function fits a `RandomForestClassifier` to the training data to derive feature importances. It then selects the top N features (default is 10) that have the highest importance scores.
- **Filtering Features:** The `filter_features` function trims the datasets to retain only the top features along with the `id` and `type` columns (for training data).
- **Saving Final Datasets:** The final processed datasets with selected features are saved as `processed_train.csv` and `processed_test.csv`, aligning with the project's requirements.
- **Error Handling:** Ensures that any missing columns or unexpected issues during feature selection and filtering are captured and logged.

---

### Summary of the Feature Engineering Phase

1. **Derived Features:** Created interaction, polynomial, and ratio features to enrich the dataset's feature space.
2. **Categorical Encoding:** Applied One-Hot Encoding to the `color` feature, ensuring consistency between training and testing datasets.
3. **Scaling:** Standardized numerical features to have a mean of 0 and a standard deviation of 1, enhancing model performance.
4. **Feature Selection:** Employed a `RandomForestClassifier` to identify and retain the top 10 most influential features, reducing dimensionality and potential overfitting.

Throughout the process, robust logging and error handling mechanisms were implemented to ensure traceability and facilitate debugging. The final processed datasets are now ready for the subsequent phase: Model Building, Validation, and Prediction.