[
    {
        "task": "Create Interaction and Derived Features",
        "tools, involved features and correct parameters": [
            "Tools: Pandas",
            "Involved Features: Numerical - bone_length, rotting_flesh, hair_length, has_soul",
            "Actions:",
            "- Create interaction terms such as hair_length_has_soul (hair_length \u00d7 has_soul) and bone_length_rotting_flesh (bone_length \u00d7 rotting_flesh)",
            "- Generate polynomial features like bone_length_squared (bone_length\u00b2) and rotting_flesh_squared (rotting_flesh\u00b2)",
            "- Create ratio features such as bone_to_flesh_ratio (bone_length / rotting_flesh) and soul_to_hair_ratio (has_soul / hair_length)"
        ],
        "expected output or impact on data": [
            "Addition of new features: hair_length_has_soul, bone_length_rotting_flesh, bone_length_squared, rotting_flesh_squared, bone_to_flesh_ratio, soul_to_hair_ratio",
            "Enhanced model with more nuanced information capturing interactions and non-linear relationships"
        ],
        "constraints": [
            "Prevent multicollinearity by ensuring interaction terms do not overly correlate with existing features",
            "Avoid introducing missing or infinite values through derived feature calculations"
        ]
    },
    {
        "task": "Encode Categorical Variables",
        "tools, involved features and correct parameters": [
            "Tools: Pandas (get_dummies), category_encoders for target encoding (optional)",
            "Involved Features: Categorical - color",
            "Actions:",
            "- Apply One-Hot Encoding to color, creating binary indicators like color_clear, color_green, color_black, color_white, color_blue",
            "- Optionally, apply Target Encoding by replacing color categories with mean target values, ensuring techniques to prevent overfitting"
        ],
        "expected output or impact on data": [
            "Transformed color feature into numerical format with multiple binary columns or a single encoded numerical column",
            "Improved model's ability to leverage categorical information effectively"
        ],
        "constraints": [
            "Choose between One-Hot Encoding and Target Encoding based on data distribution and model requirements",
            "Limit the number of dummy variables to prevent dimensionality explosion",
            "Handle unseen categories in the test set consistently"
        ]
    },
    {
        "task": "Normalize and Scale Numerical Features",
        "tools, involved features and correct parameters": [
            "Tools: Scikit-learn (MinMaxScaler, StandardScaler)",
            "Involved Features: Numerical - bone_length, rotting_flesh, hair_length, has_soul, and all newly created interaction and derived features",
            "Actions:",
            "- Apply Min-Max Scaling to scale features between 0 and 1 for algorithms like Neural Networks",
            "- Apply Standardization (Z-score Scaling) to transform features to have a mean of 0 and standard deviation of 1 for algorithms like SVM and Logistic Regression",
            "- Fit scalers on training data and transform both training and test datasets using the same parameters"
        ],
        "expected output or impact on data": [
            "All numerical features are scaled to comparable ranges, enhancing model performance and training efficiency"
        ],
        "constraints": [
            "Perform scaling after splitting data to avoid data leakage",
            "Use the same scaling parameters for both training and test datasets"
        ]
    },
    {
        "task": "Feature Selection and Dimensionality Reduction",
        "tools, involved features and correct parameters": [
            "Tools: Pandas, Scikit-learn (RandomForestClassifier, GradientBoostingClassifier, PCA)",
            "Involved Features: All numerical and encoded categorical features",
            "Actions:",
            "- Conduct correlation analysis (Pearson/Spearman) to identify features strongly correlated with type and remove highly correlated redundant features",
            "- Utilize tree-based models like Random Forest to derive feature importance scores and select top features",
            "- Apply Lasso (L1) regularization to identify significant features",
            "- Optionally, perform Principal Component Analysis (PCA) to reduce dimensionality while retaining variance"
        ],
        "expected output or impact on data": [
            "A refined set of relevant and non-redundant features for modeling",
            "Reduced computational complexity and potentially enhanced model performance"
        ],
        "constraints": [
            "Ensure feature selection methods are consistent across training and test datasets",
            "Maintain interpretability if model explainability is required",
            "Avoid discarding features solely based on correlation without considering their interaction effects"
        ]
    }
]