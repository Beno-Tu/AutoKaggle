### TASK 1: Comprehensive Univariate Analysis of Numerical Features

**THOUGHT PROCESS:**

For the comprehensive univariate analysis of numerical features, the goal is to generate descriptive statistics and assess the distribution characteristics of each numerical feature. This includes calculating metrics like mean, median, standard deviation, min, max, quartiles, skewness, and kurtosis. Additionally, visualizations such as histograms and box plots will help identify the distribution shape and potential outliers. Given the constraints, we'll ensure that all numerical features are correctly typed and focus on significant anomalies without overcomplicating the visualizations.

**CODE:**
```python
import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Define the data directories
DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'
CLEANED_DATA_DIR = DATA_DIR
IMAGE_DIR = os.path.join(DATA_DIR, 'deep_eda', 'images')

# Create the image directory if it doesn't exist
os.makedirs(IMAGE_DIR, exist_ok=True)

# Load the cleaned training dataset
cleaned_train_path = os.path.join(CLEANED_DATA_DIR, 'cleaned_train.csv')
train_df = pd.read_csv(cleaned_train_path)

# Define numerical columns
numerical_cols = [
    'Previous qualification (grade)', 'Admission grade',
    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate', 'Inflation rate', 'GDP', 'Age at enrollment'
]

# Ensure all numerical features are correctly typed
train_df[numerical_cols] = train_df[numerical_cols].apply(pd.to_numeric, errors='coerce')

# Initialize a dictionary to store descriptive statistics
descriptive_stats = {}

print("Comprehensive Univariate Analysis of Numerical Features:\n")

for col in numerical_cols:
    if col in train_df.columns:
        col_data = train_df[col].dropna()
        stats = {
            'Mean': col_data.mean(),
            'Median': col_data.median(),
            'Standard Deviation': col_data.std(),
            'Min': col_data.min(),
            '25%': col_data.quantile(0.25),
            '50%': col_data.quantile(0.50),
            '75%': col_data.quantile(0.75),
            'Max': col_data.max(),
            'Skewness': skew(col_data),
            'Kurtosis': kurtosis(col_data)
        }
        descriptive_stats[col] = stats
        # Print the descriptive statistics
        print(f"Feature: {col}")
        for stat_name, value in stats.items():
            print(f"  {stat_name}: {value:.4f}")
        print("\n")

        # Generate Histogram
        plt.figure(figsize=(8, 4))
        sns.histplot(col_data, kde=True, bins=30, color='blue', edgecolor='black', stat="density")
        plt.title(f'Histogram of {col}')
        plt.xlabel(col)
        plt.ylabel('Density')
        histogram_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_histogram.png')
        plt.savefig(histogram_path, bbox_inches='tight')
        plt.close()
        print(f"Saved histogram for '{col}' at {histogram_path}")

        # Generate Box Plot
        plt.figure(figsize=(6, 4))
        sns.boxplot(x=col_data, color='orange')
        plt.title(f'Box Plot of {col}')
        plt.xlabel(col)
        boxplot_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_boxplot.png')
        plt.savefig(boxplot_path, bbox_inches='tight')
        plt.close()
        print(f"Saved box plot for '{col}' at {boxplot_path}\n")

# Optionally, save the descriptive statistics to a CSV file
descriptive_stats_df = pd.DataFrame(descriptive_stats).T
descriptive_stats_df.to_csv(os.path.join(IMAGE_DIR, 'numerical_descriptive_stats.csv'))
print("Descriptive statistics saved to 'numerical_descriptive_stats.csv'")
```

**EXPLANATION:**

The code performs a comprehensive univariate analysis on all numerical features in the training dataset. It starts by loading the cleaned training data and ensuring that all numerical columns are of the correct data type. For each numerical feature, it calculates descriptive statistics such as mean, median, standard deviation, min, max, quartiles, skewness, and kurtosis. These statistics are printed for review and saved into a CSV file for future reference.

Additionally, for each numerical feature, the code generates and saves a histogram and a box plot to visualize the distribution and identify potential outliers. The images are saved in the specified `deep_eda/images/` directory with clear and meaningful filenames. After saving each plot, `plt.close()` is called to free up memory, adhering to the constraints.

---

### TASK 2: Detailed Univariate Analysis of Categorical Features

**THOUGHT PROCESS:**

The goal of this task is to analyze each categorical feature by examining the frequency and proportion of each category. This helps in identifying class imbalances and potential data quality issues like unexpected categories or inconsistent labeling. Given the constraints, we will focus on categories that are underrepresented (e.g., less than 5%) and ensure that category labels are consistent.

**CODE:**
```python
# Define categorical columns
categorical_cols = [
    'Marital status', 'Application mode', 'Course', 'Previous qualification',
    'Nacionality', "Mother's qualification", "Father's qualification",
    "Mother's occupation", "Father's occupation", 'Displaced',
    'Educational special needs', 'Debtor', 'Tuition fees up to date',
    'Gender', 'Scholarship holder', 'International'
]

print("Detailed Univariate Analysis of Categorical Features:\n")

for col in categorical_cols:
    if col in train_df.columns:
        col_data = train_df[col].dropna()
        value_counts = col_data.value_counts()
        proportions = col_data.value_counts(normalize=True) * 100
        category_df = pd.DataFrame({
            'Count': value_counts,
            'Proportion (%)': proportions.round(2)
        })
        print(f"Feature: {col}")
        print(category_df)
        print("\n")

        # Save the value counts to a CSV file
        category_csv_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_value_counts.csv')
        category_df.to_csv(category_csv_path)
        print(f"Saved value counts for '{col}' at {category_csv_path}")

        # Identify categories with less than 5% proportion
        low_freq = category_df[category_df['Proportion (%)'] < 5]
        if not low_freq.empty:
            print(f"Categories in '{col}' with less than 5% proportion:")
            print(low_freq)
            print("\n")

        # Generate Bar Plot
        plt.figure(figsize=(10, 6))
        sns.countplot(data=train_df, x=col, order=value_counts.index, palette='viridis')
        plt.title(f'Count Plot of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        barplot_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_countplot.png')
        plt.savefig(barplot_path)
        plt.close()
        print(f"Saved count plot for '{col}' at {barplot_path}\n")
```

**EXPLANATION:**

This script conducts a detailed univariate analysis of all categorical features in the training dataset. For each categorical column, it calculates the count and proportion of each category, printing the results for review. These counts are also saved as CSV files in the `deep_eda/images/` directory for documentation purposes.

The code identifies and prints categories that constitute less than 5% of the data, highlighting potential class imbalances or rare categories that might need special attention during modeling. Additionally, for each categorical feature, a count plot is generated and saved to visualize the distribution of categories. The plots are saved with clear filenames, and `plt.close()` is used after each save to manage memory efficiently.

---

### TASK 3: Bivariate Analysis Between Features and Target Variable

**THOUGHT PROCESS:**

The objective here is to understand the relationship between each feature and the target variable (`Target`). For numerical features, we'll segment the data by target classes and compute descriptive statistics. For categorical features, we'll examine the distribution of categories across target classes and calculate association metrics like Chi-Square for categorical and correlation coefficients for numerical features. This analysis will help identify which features have significant relationships with the target, guiding feature engineering and selection.

**CODE:**
```python
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder

# Define target variable
target = 'Target'
if target not in train_df.columns:
    print(f"Target variable '{target}' not found in the dataset.")
else:
    print("Bivariate Analysis Between Features and Target Variable:\n")

    # Initialize dictionaries to store association metrics
    numerical_associations = {}
    categorical_associations = {}

    # Bivariate Analysis for Numerical Features
    print("Numerical Features Analysis:\n")
    for col in numerical_cols:
        if col in train_df.columns:
            print(f"Feature: {col}")
            # Group data by target and calculate descriptive stats
            grouped = train_df.groupby(target)[col].describe()
            print(grouped)
            print("\n")

            # Calculate Pearson Correlation with Target
            # Encode target variable
            le = LabelEncoder()
            target_encoded = le.fit_transform(train_df[target])
            correlation = train_df[col].corr(pd.Series(target_encoded))
            numerical_associations[col] = correlation
            print(f"Pearson Correlation between '{col}' and '{target}': {correlation:.4f}\n")

            # Generate Box Plot
            plt.figure(figsize=(8, 6))
            sns.boxplot(x=target, y=col, data=train_df, palette='Set2')
            plt.title(f'Box Plot of {col} by {target}')
            plt.xlabel(target)
            plt.ylabel(col)
            boxplot_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_vs_{target}_boxplot.png')
            plt.savefig(boxplot_path, bbox_inches='tight')
            plt.close()
            print(f"Saved box plot for '{col}' vs '{target}' at {boxplot_path}\n")

    # Bivariate Analysis for Categorical Features
    print("Categorical Features Analysis:\n")
    for col in categorical_cols:
        if col in train_df.columns:
            print(f"Feature: {col}")
            contingency_table = pd.crosstab(train_df[col], train_df[target])
            print(contingency_table)
            print("\n")

            # Perform Chi-Square Test
            chi2, p, dof, ex = chi2_contingency(contingency_table)
            categorical_associations[col] = p
            print(f"Chi-Square Test for '{col}' and '{target}': p-value = {p:.4f}\n")

            # Generate Stacked Bar Plot
            contingency_table.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')
            plt.title(f'Stacked Bar Plot of {col} by {target}')
            plt.xlabel(col)
            plt.ylabel('Count')
            plt.xticks(rotation=45, ha='right')
            plt.legend(title=target)
            plt.tight_layout()
            stacked_bar_path = os.path.join(IMAGE_DIR, f'{col.replace(" ", "_")}_vs_{target}_stacked_bar.png')
            plt.savefig(stacked_bar_path)
            plt.close()
            print(f"Saved stacked bar plot for '{col}' vs '{target}' at {stacked_bar_path}\n")

    # Summary of Associations
    print("Summary of Feature Associations with Target Variable:\n")

    # Numerical Features with significant correlations
    print("Numerical Features Correlation with Target:")
    significant_numerical = {k: v for k, v in numerical_associations.items() if abs(v) > 0.1}
    for feature, corr_value in significant_numerical.items():
        print(f"  {feature}: Pearson Correlation = {corr_value:.4f}")
    print("\n")

    # Categorical Features with significant associations (p-value < 0.05)
    print("Categorical Features Chi-Square Test p-values:")
    significant_categorical = {k: v for k, v in categorical_associations.items() if v < 0.05}
    for feature, p_value in significant_categorical.items():
        print(f"  {feature}: p-value = {p_value:.4f}")
    print("\n")

    # Save association metrics to CSV files
    numerical_assoc_df = pd.DataFrame.from_dict(numerical_associations, orient='index', columns=['Pearson Correlation'])
    numerical_assoc_df.to_csv(os.path.join(IMAGE_DIR, 'numerical_target_correlations.csv'))
    categorical_assoc_df = pd.DataFrame.from_dict(categorical_associations, orient='index', columns=['Chi2 p-value'])
    categorical_assoc_df.to_csv(os.path.join(IMAGE_DIR, 'categorical_target_chi2_pvalues.csv'))
    print("Association metrics saved to 'numerical_target_correlations.csv' and 'categorical_target_chi2_pvalues.csv'")
```

**EXPLANATION:**

This script performs a bivariate analysis to explore the relationships between each feature and the target variable (`Target`). 

- **Numerical Features:** For each numerical feature, the code groups the data by the target classes and computes descriptive statistics. It then calculates the Pearson correlation coefficient between the numerical feature and the encoded target variable to quantify the strength and direction of the relationship. Box plots are generated to visually assess how the distribution of each numerical feature varies across different target classes. Significant correlations (absolute correlation greater than 0.1) are highlighted in the summary.

- **Categorical Features:** For each categorical feature, a contingency table is created to show the distribution of categories across target classes. The Chi-Square test is performed to determine if there is a significant association between the categorical feature and the target variable. Stacked bar plots are generated for visual inspection of these relationships. Features with p-values less than 0.05 are considered to have significant associations and are highlighted in the summary.

All association metrics are saved as CSV files for documentation and further analysis. Visualizations are saved with descriptive filenames, and `plt.close()` is used after each plot to manage memory efficiently.

---

### TASK 4: Correlation and Multivariate Analysis of Numerical Features

**THOUGHT PROCESS:**

The aim is to examine the pairwise correlations among numerical features to identify multicollinearity issues. A correlation matrix will highlight strong relationships (|r| > 0.7), and a Variance Inflation Factor (VIF) analysis will quantify the degree of multicollinearity for each feature. Features with high VIF scores may be candidates for removal or transformation to reduce redundancy and improve model performance.

**CODE:**
```python
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

print("Correlation and Multivariate Analysis of Numerical Features:\n")

# Compute the correlation matrix
corr_matrix = train_df[numerical_cols].corr(method='pearson')

# Select correlations with absolute value greater than 0.7
high_corr = corr_matrix[(corr_matrix.abs() > 0.7) & (corr_matrix.abs() < 1.0)]

# Display high correlations
print("Pairs of Numerical Features with |Pearson Correlation| > 0.7:\n")
high_corr_pairs = high_corr.stack().reset_index()
high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Pearson Correlation']
print(high_corr_pairs)
print("\n")

# Save high correlations to CSV
high_corr_pairs.to_csv(os.path.join(IMAGE_DIR, 'high_correlations_numerical_features.csv'), index=False)
print("High correlation pairs saved to 'high_correlations_numerical_features.csv'\n")

# Calculate Variance Inflation Factor (VIF)
# VIF is calculated for each feature by regressing it against all other features
# First, drop any columns with missing values
vif_data = train_df[numerical_cols].dropna()

# Add a constant term for VIF calculation
X = sm.add_constant(vif_data)

vif = pd.DataFrame()
vif['Feature'] = numerical_cols
vif['VIF'] = [variance_inflation_factor(X.values, i+1) for i in range(len(numerical_cols))]  # +1 to skip the constant

print("Variance Inflation Factor (VIF) for Numerical Features:\n")
print(vif)
print("\n")

# Save VIF results to CSV
vif.to_csv(os.path.join(IMAGE_DIR, 'vif_numerical_features.csv'), index=False)
print("VIF results saved to 'vif_numerical_features.csv'\n")

# Recommendations based on VIF
high_vif = vif[vif['VIF'] > 5]
if not high_vif.empty:
    print("Features with VIF > 5, indicating multicollinearity:\n")
    print(high_vif)
    print("\nConsider removing or combining these features to reduce multicollinearity.\n")
else:
    print("No features with VIF > 5 detected. Multicollinearity is not a concern.\n")

# Visualize the correlation matrix with a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmax=1, vmin=-1, linewidths=0.5, annot_kws={"size":8})
plt.title('Correlation Matrix of Numerical Features')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
heatmap_path = os.path.join(IMAGE_DIR, 'numerical_features_correlation_heatmap.png')
plt.savefig(heatmap_path, bbox_inches='tight')
plt.close()
print(f"Saved correlation heatmap at {heatmap_path}\n")
```

**EXPLANATION:**

This script conducts a correlation and multivariate analysis of numerical features to identify multicollinearity issues that could affect model performance.

1. **Correlation Matrix:** The Pearson correlation matrix is computed for all numerical features. Pairs of features with an absolute correlation greater than 0.7 are identified and saved to a CSV file. This helps in spotting highly correlated feature pairs that may need to be addressed.

2. **Variance Inflation Factor (VIF):** VIF quantifies the severity of multicollinearity in regression analysis. Each numerical feature's VIF is calculated by regressing it against all other numerical features. A VIF value greater than 5 suggests significant multicollinearity, indicating that the feature may be redundant and could potentially be removed or combined with other features. The VIF results are saved for documentation.

3. **Recommendations:** If any features have a VIF greater than 5, they are listed with a recommendation to consider their removal or combination to mitigate multicollinearity.

4. **Correlation Heatmap:** A heatmap visualizing the entire correlation matrix is generated and saved. This provides a comprehensive view of how all numerical features relate to each other, facilitating the identification of patterns and relationships.

All findings are saved appropriately in the `deep_eda/images/` directory, and `plt.close()` ensures that memory is managed efficiently after each plot is saved.

---