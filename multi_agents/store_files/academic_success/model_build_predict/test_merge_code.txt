```python
import pandas as pd
import numpy as np
from pathlib import Path

# Define directories
DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'
MODEL_DIR = DATA_DIR + 'model_build_predict/'
IMAGE_DIR = MODEL_DIR + 'images/'

# Ensure directories exist
Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)
Path(IMAGE_DIR).mkdir(parents=True, exist_ok=True)

# Load processed data
processed_train_path = DATA_DIR + 'processed_train.csv'
processed_test_path = DATA_DIR + 'processed_test.csv'

train_df = pd.read_csv(processed_train_path)
test_df = pd.read_csv(processed_test_path)

# Make copies to avoid modifying original data
X_train = train_df.copy()
y_train = X_train.pop('Target')  # Separate target
X_test = test_df.copy()

# Remove 'id' column as it is not used for training
X_train = X_train.drop(columns=['id'], errors='ignore')
X_test = X_test.drop(columns=['id'], errors='ignore')

print("Target variable separated and 'id' column removed from training and test sets.")

# Identify non-numeric columns
non_numeric_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()
print(f"Non-numeric columns to be removed: {non_numeric_cols}")

# Remove non-numeric columns from training and test sets
X_train = X_train.drop(columns=non_numeric_cols, errors='ignore')
X_test = X_test.drop(columns=non_numeric_cols, errors='ignore')

print("Non-numeric columns removed from training and test sets.")

# Ensure feature alignment
train_features = set(X_train.columns)
test_features = set(X_test.columns)

missing_in_test = train_features - test_features
missing_in_train = test_features - train_features

if missing_in_test:
    X_test = X_test.drop(columns=list(missing_in_test))
    print(f"Dropped columns from test set not present in training set: {missing_in_test}")

if missing_in_train:
    X_train = X_train.drop(columns=list(missing_in_train))
    print(f"Dropped columns from training set not present in test set: {missing_in_train}")

print("Feature alignment between training and test sets ensured.")

# Save the prepared datasets for verification
prepared_train_path = DATA_DIR + 'model_build_predict/ready_train.csv'
prepared_test_path = DATA_DIR + 'model_build_predict/ready_test.csv'

X_train.to_csv(prepared_train_path, index=False)
X_test.to_csv(prepared_test_path, index=False)

print(f"Prepared datasets saved at '{prepared_train_path}' and '{prepared_test_path}'.")


from sklearn.preprocessing import StandardScaler

# Load prepared data
prepared_train_path = DATA_DIR + 'model_build_predict/ready_train.csv'
prepared_test_path = DATA_DIR + 'model_build_predict/ready_test.csv'

X_train = pd.read_csv(prepared_train_path)
X_test = pd.read_csv(prepared_test_path)

print("Prepared training and test datasets loaded.")

# Verify all remaining features are numeric
numeric_train = X_train.select_dtypes(include=[np.number])
numeric_test = X_test.select_dtypes(include=[np.number])

if numeric_train.shape[1] != X_train.shape[1] or numeric_test.shape[1] != X_test.shape[1]:
    non_numeric_train = X_train.columns.difference(numeric_train.columns)
    non_numeric_test = X_test.columns.difference(numeric_test.columns)
    print(f"Warning: Non-numeric columns detected in training set: {non_numeric_train}")
    print(f"Warning: Non-numeric columns detected in test set: {non_numeric_test}")
else:
    print("All features are numeric in both training and test sets.")

# Initialize StandardScaler
scaler = StandardScaler()

# Fit scaler on training data
scaler.fit(X_train)

# Transform both training and test data
X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

print("Feature scaling applied to training and test sets.")

# Verify scaling by checking mean and standard deviation
print("\nTraining Data - Mean after scaling:\n", X_train_scaled.mean())
print("\nTraining Data - Std Dev after scaling:\n", X_train_scaled.std())

print("\nTest Data - Mean after scaling:\n", X_test_scaled.mean())
print("\nTest Data - Std Dev after scaling:\n", X_test_scaled.std())

# Save the scaled datasets for modeling
scaled_train_path = DATA_DIR + 'model_build_predict/scaled_train.csv'
scaled_test_path = DATA_DIR + 'model_build_predict/scaled_test.csv'

X_train_scaled.to_csv(scaled_train_path, index=False)
X_test_scaled.to_csv(scaled_test_path, index=False)

print(f"Scaled datasets saved at '{scaled_train_path}' and '{scaled_test_path}'.")


# Assume that 'train_and_validation_and_select_the_best_model' is pre-imported

# Load scaled training data
scaled_train_path = DATA_DIR + 'model_build_predict/scaled_train.csv'
X_train_scaled = pd.read_csv(scaled_train_path)
y_train = pd.read_csv(DATA_DIR + 'processed_train.csv')['Target']  # Reload Target from original processed_train.csv

print("Scaled training data and target variable loaded.")

# Define problem type and selected models
problem_type = "multiclass"
selected_models = ["random forest", "logistic regression"]  # Reduced number of models

print(f"Starting model training with models: {selected_models} for a {problem_type} problem.")

# Train models and select the best one
best_model = train_and_validation_and_select_the_best_model(
    X=X_train_scaled,
    y=y_train,
    problem_type=problem_type,
    selected_models=selected_models
)

print(f"Best model selected: {best_model}")

# Save the best model for future use (optional)
# This assumes that the best_model has a method to save itself, e.g., using joblib or pickle
# import joblib
# model_path = MODEL_DIR + 'best_model.pkl'
# joblib.dump(best_model, model_path)
# print(f"Best model saved at '{model_path}'.")


# Load scaled test data
scaled_test_path = DATA_DIR + 'model_build_predict/scaled_test.csv'
X_test_scaled = pd.read_csv(scaled_test_path)

print("Scaled test data loaded.")

# Generate predictions using the best model
print("Generating predictions on the test set.")
predictions = best_model.predict(X_test_scaled)

# Load the original test dataframe to retrieve 'id' for submission
original_test_path = DATA_DIR + 'processed_test.csv'
test_df_original = pd.read_csv(original_test_path)
submission_ids = test_df_original['id']

# Create submission DataFrame
submission_df = pd.DataFrame({
    'id': submission_ids,
    'Target': predictions
})

# Ensure no missing values in the submission
if submission_df.isnull().values.any():
    print("Warning: Missing values detected in the submission. Filling missing values with 'Unknown'.")
    submission_df = submission_df.fillna('Unknown')

# Verify the format of 'Target' labels
expected_labels = ['dropout', 'enrolled', 'graduate']
if not submission_df['Target'].isin(expected_labels).all():
    print("Warning: Some 'Target' predictions are outside the expected labels. Please verify model outputs.")

import os

# Define the competition root directory by navigating one level up from MODEL_DIR
competition_root = os.path.abspath(os.path.join(MODEL_DIR, '..'))

# Define submission path at the competition root directory
submission_path = os.path.join(competition_root, 'submission.csv')

# Ensure the competition root directory exists
os.makedirs(competition_root, exist_ok=True)

# Save the submission file
submission_df.to_csv(submission_path, index=False)

print(f"Submission file 'submission.csv' created successfully at '{submission_path}'.")
```