[
    {
        "role": "user",
        "content": "You are an excellent developer.\nYou are skilled at writing and implementing code according to plan.\n when you are writing code, you should follow the plan and the following constraints.\n\n# CONSTRAINTS #\n\n## DATA HANDLING ##\n1. Data Loading:\n   - Always load data files from the `/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/` directory.\n   - Use specific files for each phase:\n     - Data Cleaning: `train.csv` and `test.csv`\n     - Feature Engineering: `cleaned_train.csv` and `cleaned_test.csv`\n     - Model Building, Validation, and Prediction: `processed_train.csv` and `processed_test.csv`\n\n2. Data Saving:\n   - Save image files in the `/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/data_cleaning/images/` directory.\n   - Save data files in the `/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/` directory.\n   - Use clear, meaningful names for image files that reflect their content.\n   - Do NOT use special characters like `/` or spaces in file names.\n   - Save specific files for each phase:\n     - Data Cleaning: `cleaned_train.csv` and `cleaned_test.csv`\n     - Feature Engineering: `processed_train.csv` and `processed_test.csv`\n\n3. Data Processing:\n   - Always work on a copy of the DataFrame, not the original.\n   - Ensure correct data types for all columns before any operations.\n   - Apply consistent modifications (e.g., scaling, encoding) to both training and test sets.\n   - Take care with target-dependent operations on the test set, which lacks the target variable.\n   - Do NOT modify Id-type columns.\n\n## CODING PRACTICES ##\n1. General Rules:\n   - Use `print()` for outputting values.\n   - Avoid writing `assert` statements.\n\n2. Visualization:\n   - Use `plt.close()` after saving each image.\n   - Limit EDA visualizations to 10 or fewer, focusing on the most insightful.\n   - Optimize for large datasets (e.g., `annot=False` in seaborn heatmaps).\n\n3. Efficiency:\n   - Prioritize runtime efficiency, especially for:\n     - Data visualization\n     - Large dataset handling\n     - Complex algorithms\n\n## EXAMPLES ##\n- Before calculating a correlation matrix, ensure all data is numerical. Handle non-numerical data appropriately.\n- Verify consistent data types across columns before merging or joining operations.\n\nRemember: Always consider resource constraints and prioritize efficiency in your code.\n"
    },
    {
        "role": "user",
        "content": "\n# CONTEXT #\n\nI am working on a data science competition called \"classification_with_an_academic_success_dataset\". \nI plan to divide the task into the following phases and complete them in order:\n1. Understand Background\n2. Preliminary Exploratory Data Analysis\n3. Data Cleaning\n4. In-depth Exploratory Data Analysis\n5. Feature Engineering\n6. Model Building, Validation, and Prediction\nCurrently, I am at phase: Data Cleaning.\nIn Data Cleaning, you already have file `train.csv' and `test.csv`. In this phase, the overall goal is to do data cleaning to them to get `cleaned_train.csv` and `cleaned_test.csv`\n\n#############\n# COMPETITION INFORMATION #\n# Competition Information\n## 1. Competition Overview\nThe competition is part of the Kaggle Playground Series, aimed at predicting the academic risk of students in higher education. Participants will classify students into categories of dropout, enrolled, and graduate, with accuracy as the evaluation metric.\n\n## 2. Files\n- **train.csv**: Training dataset containing the `Target` variable for model training.\n- **test.csv**: Test dataset without the `Target` variable for which predictions must be made.\n- **sample_submission.csv**: Template file demonstrating the required format for submissions.\n\n## 3. Problem Definition\nThe objective is to classify students based on their academic risk using various features from the dataset.\n\n## 4. Data Information\n### 4.1 Data type\n#### 4.1.1 ID type\n- `id`: Unique identifier for each student (not used in model training).\n\n#### 4.1.2 Numerical type\n- `Previous qualification (grade)`\n- `Admission grade`\n- `Curricular units (credited/enrolled/evaluations/approved/grade)`\n- `Unemployment rate`\n- `Inflation rate`\n- `GDP`\n\n#### 4.1.3 Categorical type\n- `Marital status`\n- `Application mode`\n- `Course`\n- `Previous qualification`\n- `Nacionality`\n- `Mother's qualification`\n- `Father's qualification`\n- `Mother's occupation`\n- `Father's occupation`\n- `Displaced`\n- `Educational special needs`\n- `Debtor`\n- `Tuition fees up to date`\n- `Gender`\n- `Scholarship holder`\n- `International`\n- `Target`\n\n#### 4.1.4 Datetime type\n- None\n\n### 4.2 Detailed data description\nThe dataset includes features related to students' demographics, academic history, and socio-economic factors, providing a comprehensive view of their educational backgrounds and current standing.\n\n## 5. Target Variable\n- **Target**: A categorical variable indicating the academic risk classification (dropout, enrolled, graduate). Present in the training set but absent in the test set.\n\n## 6. Evaluation Metrics\nThe primary evaluation metric is **accuracy**, representing the proportion of true results among all predictions made.\n\n## 7. Submission Format\nThe submission file must include a header and follow this format:\n```\nid,Target\n76518,Graduate\n76519,Graduate\n76520,Graduate\n```\n\n## 8. Other Key Aspects\nThe dataset is synthetically generated, encouraging participants to explore the original dataset for insights into feature distributions. The competition runs from June 1 to June 30, 2024, with specific deadlines for submissions.\n\n#############\n# PLAN #\n## PLAN\n\n### STEP 1\nTask: **Handle Missing Values**\nTools, involved features and correct parameters:\n- **Tools:**\n  - `fill_missing_values`\n    - **Numerical Features:** \n      - Columns: `Previous qualification (grade)`, `Admission grade`, `Curricular units 1st sem (credited)`, `Curricular units 1st sem (enrolled)`, `Curricular units 1st sem (evaluations)`, `Curricular units 1st sem (approved)`, `Curricular units 1st sem (grade)`, `Curricular units 1st sem (without evaluations)`, `Curricular units 2nd sem (credited)`, `Curricular units 2nd sem (enrolled)`, `Curricular units 2nd sem (evaluations)`, `Curricular units 2nd sem (approved)`, `Curricular units 2nd sem (grade)`, `Curricular units 2nd sem (without evaluations)`, `Unemployment rate`, `Inflation rate`, `GDP`\n      - Method: `median`\n    - **Categorical Features:** \n      - Columns: `Marital status`, `Application mode`, `Course`, `Previous qualification`, `Nationality`, `Mother's qualification`, `Father's qualification`, `Mother's occupation`, `Father's occupation`, `Displaced`, `Educational special needs`, `Debtor`, `Tuition fees up to date`, `Gender`, `Scholarship holder`, `International`\n      - Method: `mode`\n  - `remove_columns_with_missing_data`\n    - **Parameters:** \n      - `thresh`: `0.6` (remove columns with more than 60% missing values)\n- **Features Involved:** All features in both `train.csv` and `test.csv`\n\nExpected output or Impact on data:\n- Datasets (`cleaned_train.csv` and `cleaned_test.csv`) with missing values imputed appropriately.\n- Removal of columns with excessive missing values (above 60%), reducing noise and potential biases.\n\nConstraints:\n- Prevent introduction of significant bias through imputation.\n- Maintain consistency in handling missing values across both training and test datasets to avoid data leakage.\n\n---\n\n### STEP 2\nTask: **Detect and Handle Outliers in Numerical Features**\nTools, involved features and correct parameters:\n- **Tools:**\n  - `detect_and_handle_outliers_iqr`\n    - **Parameters:**\n      - `factor`: `1.5` (to identify outliers)\n      - `method`: `clip` (to cap outlier values)\n- **Features Involved:** \n  - Numerical Features:\n    - `Previous qualification (grade)`\n    - `Admission grade`\n    - `Curricular units 1st sem (credited)`\n    - `Curricular units 1st sem (enrolled)`\n    - `Curricular units 1st sem (evaluations)`\n    - `Curricular units 1st sem (approved)`\n    - `Curricular units 1st sem (grade)`\n    - `Curricular units 1st sem (without evaluations)`\n    - `Curricular units 2nd sem (credited)`\n    - `Curricular units 2nd sem (enrolled)`\n    - `Curricular units 2nd sem (evaluations)`\n    - `Curricular units 2nd sem (approved)`\n    - `Curricular units 2nd sem (grade)`\n    - `Curricular units 2nd sem (without evaluations)`\n    - `Unemployment rate`\n    - `Inflation rate`\n    - `GDP`\n\nExpected output or Impact on data:\n- Numerical features with outliers capped at the upper and lower bounds based on the IQR method.\n- Enhanced robustness and performance of subsequent models by reducing the impact of extreme values.\n\nConstraints:\n- Do not remove outliers from the test dataset to maintain consistency.\n- Ensure clipping preserves the overall data distribution without artificially compressing feature ranges.\n\n---\n\n### STEP 3\nTask: **Normalize and Standardize Categorical Features**\nTools, involved features and correct parameters:\n- **Tools:**\n  - `convert_data_types`\n    - **Parameters:**\n      - **Categorical Columns:** \n        - `Marital status`, `Application mode`, `Course`, `Previous qualification`, `Nationality`, `Mother's qualification`, `Father's qualification`, `Mother's occupation`, `Father's occupation`, `Displaced`, `Educational special needs`, `Debtor`, `Tuition fees up to date`, `Gender`, `Scholarship holder`, `International`\n      - `target_type`: `str`\n- **Methods:**\n  - Utilize pandas string methods to:\n    - Convert all categorical strings to lowercase.\n    - Strip leading and trailing whitespaces.\n    - Correct common typos based on predefined mappings or manual inspection.\n- **Features Involved:** \n  - All categorical features listed above in both `train.csv` and `test.csv`\n\nExpected output or Impact on data:\n- Categorical features are standardized with consistent formatting, eliminating discrepancies such as case sensitivity and unwanted spaces.\n- Enhanced data quality, facilitating smoother downstream processing and modeling.\n\nConstraints:\n- Avoid merging distinct categories inadvertently during standardization.\n- Preserve the semantic meaning of each category to maintain data integrity.\n\n---\n\n### STEP 4\nTask: **Remove Duplicate Rows and Convert Data Types**\nTools, involved features and correct parameters:\n- **Tools:**\n  - `remove_duplicates`\n    - **Parameters:**\n      - `columns`: All columns except `id`\n      - `keep`: `first`\n  - `convert_data_types`\n    - **Parameters:**\n      - **Numerical Columns:** Ensure data types are `int` or `float` as appropriate.\n      - **Categorical Columns:** Ensure data types are `str`.\n      - **ID Column:** Convert to `str` to preserve formatting.\n- **Features Involved:** \n  - All features in both `train.csv` and `test.csv`\n\nExpected output or Impact on data:\n- Removal of duplicate rows, ensuring each `id` remains unique.\n- All features possess appropriate and consistent data types, enhancing data integrity and readiness for analysis and modeling.\n\nConstraints:\n- Preserve the unique identifier (`id`) to maintain traceability.\n- Prevent loss of information by avoiding inappropriate type conversions, such as removing leading zeros in `id` by keeping it as `str`.\n\n---\n\n#############\n# TASK #\n\nDevelop an efficient solution based on the Planner's provided plan:\n1. Implement specific tasks and methods outlined in the plan\n2. Ensure code is clear, concise, and well-documented\n3. Utilize available tools by calling them with correct parameters\n4. Consider data types, project requirements, and resource constraints\n5. Write code that is easily understandable by others\n\nRemember to balance efficiency with readability and maintainability.\n\n\n#############\n# RESPONSE: BLOCK (CODE & EXPLANATION) #\nTASK 1:\nTHOUGHT PROCESS:\n[Explain your approach and reasoning]\nCODE:\n```python\n[code]\n```\nEXPLANATION:\n[Brief explanation of the code and its purpose]\n\nTASK 2:\n[Repeat the above structure for each task/subtask]\n\n...\n\n#############\n# START CODING #\nBefore you begin, please request the following information from me:\n1. Code from previous phases\n2. All features of the data\n3. Available tools\n\nOnce you have this information, provide your complete response with code and explanations for all tasks in a single message.\n"
    },
    {
        "role": "assistant",
        "content": "Thank you for providing the comprehensive context and detailed plan for the Data Cleaning phase of your competition project. To ensure that the solution I develop aligns perfectly with your existing workflow and requirements, could you please provide the following information:\n\n1. **Code from Previous Phases:**\n   - If you've already implemented any code in the earlier phases (e.g., \"Understand Background\" or \"Preliminary Exploratory Data Analysis\"), please share that. This will help maintain consistency and avoid redundancy.\n\n2. **All Features of the Data:**\n   - A complete list of all features (columns) present in both `train.csv` and `test.csv`, including their data types. If there are any nuances or special considerations for certain features, please mention them.\n\n3. **Available Tools:**\n   - Details about any custom functions, modules, or libraries you've created or plan to use (e.g., `fill_missing_values`, `detect_and_handle_outliers_iqr`). If these tools are part of a specific Python package or are custom implementations, please provide their definitions or specifications.\n\nOnce I have this information, I'll be able to develop a clear, efficient, and well-documented solution for all the tasks outlined in your Data Cleaning plan.\n\nLooking forward to your response!"
    },
    {
        "role": "user",
        "content": "\n#############\n# CODE FROM PREVIOUS PHASE #\nThere is no code file in the previous phase.\n#############\n# TRAIN DATA WITH FEATURES #\nid,Marital status,Application mode,Application order,Course,Daytime/evening attendance,Previous qualification,Previous qualification (grade),Nacionality,Mother's qualification,Father's qualification,Mother's occupation,Father's occupation,Admission grade,Displaced,Educational special needs,Debtor,Tuition fees up to date,Gender,Scholarship holder,Age at enrollment,International,Curricular units 1st sem (credited),Curricular units 1st sem (enrolled),Curricular units 1st sem (evaluations),Curricular units 1st sem (approved),Curricular units 1st sem (grade),Curricular units 1st sem (without evaluations),Curricular units 2nd sem (credited),Curricular units 2nd sem (enrolled),Curricular units 2nd sem (evaluations),Curricular units 2nd sem (approved),Curricular units 2nd sem (grade),Curricular units 2nd sem (without evaluations),Unemployment rate,Inflation rate,GDP,Target\n\n#############\n# TEST DATA WITH FEATURES #\nid,Marital status,Application mode,Application order,Course,Daytime/evening attendance,Previous qualification,Previous qualification (grade),Nacionality,Mother's qualification,Father's qualification,Mother's occupation,Father's occupation,Admission grade,Displaced,Educational special needs,Debtor,Tuition fees up to date,Gender,Scholarship holder,Age at enrollment,International,Curricular units 1st sem (credited),Curricular units 1st sem (enrolled),Curricular units 1st sem (evaluations),Curricular units 1st sem (approved),Curricular units 1st sem (grade),Curricular units 1st sem (without evaluations),Curricular units 2nd sem (credited),Curricular units 2nd sem (enrolled),Curricular units 2nd sem (evaluations),Curricular units 2nd sem (approved),Curricular units 2nd sem (grade),Curricular units 2nd sem (without evaluations),Unemployment rate,Inflation rate,GDP\n\n# AVAILABLE TOOLS #\n## TOOL LIST ##\nYou have access to the following tools:\n['fill_missing_values', 'remove_columns_with_missing_data', 'detect_and_handle_outliers_iqr', 'convert_data_types', 'remove_duplicates']\n## USAGE INSTRUCTIONS ##\n1. These tools are pre-defined and pre-imported in the system. You do NOT need import them or implement them again.\n2. Use these tools by calling them with the correct parameters.\n3. Example: To drop specific columns, use the `remove_columns_with_missing_data` tool with appropriate parameters.\n## ADDITIONAL RESOURCES ##\nYou can also use functions from public libraries such as:\n- Pandas\n- NumPy\n- Scikit-learn\n- etc.\n## DETAILED TOOL DESCRIPTIONS ##\n## fill_missing_values\n\n**Name:** fill_missing_values  \n**Description:** Fill missing values in specified columns of a DataFrame. This tool can handle both numerical and categorical features by using different filling methods.  \n**Applicable Situations:** handle missing values in various types of features\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) where missing values should be filled.\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to use for filling missing values.\n  - **Enum:** `auto` | `mean` | `median` | `mode` | `constant`\n  - **Default:** `auto`\n- `fill_value`:\n  - **Type:** ``number` | `string` | `null``\n  - **Description:** The value to use when method is 'constant'.\n  - **Default:** `None`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully fill missing values in the specified column(s) of data  \n**Notes:**\n- The 'auto' method uses mean for numeric columns and mode for non-numeric columns.\n- Using 'mean' or 'median' on non-numeric columns will raise an error.\n- The 'mode' method uses the most frequent value, which may not always be appropriate.\n- Filling missing values can introduce bias, especially if the data is not missing completely at random.\n- Consider the impact of filling missing values on your analysis and model performance.\n\n\n## remove_columns_with_missing_data\n\n**Name:** remove_columns_with_missing_data  \n**Description:** Remove columns containing excessive missing values from a DataFrame based on a threshold. This tool provides a flexible way to clean datasets by removing columns with too many missing values, adaptable to different dataset sizes.  \n**Applicable Situations:** remove columns with excessive missing values from a dataset\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `thresh`:\n  - **Type:** `number`\n  - **Description:** The minimum proportion of missing values required to drop a column. Should be between 0 and 1.\n  - **Default:** `0.5`\n- `columns`:\n  - **Type:** ``string` | `array` | `null``\n  - **Description:** Labels of columns to consider. If not specified, all columns will be considered.\n  - **Default:** `None`\n\n**Required:** `data`  \n**Result:** Successfully remove columns containing excessive missing values from the DataFrame  \n**Notes:**\n- This method modifies the structure of your dataset by removing entire columns.\n- Setting a low threshold might result in loss of important features.\n- Setting a high threshold might retain columns with too many missing values.\n- Consider the impact of removing columns on your analysis and model performance.\n- It's often better to understand why data is missing before deciding to remove it.\n\n\n## detect_and_handle_outliers_iqr\n\n**Name:** detect_and_handle_outliers_iqr  \n**Description:** Detect and handle outliers in specified columns using the Interquartile Range (IQR) method. This tool is useful for identifying and managing extreme values in numerical features without assuming a specific distribution of the data.  \n**Applicable Situations:** detect and handle outliers in numerical features, especially when the data distribution is unknown, non-normal, or when the dataset is small or contains extreme outliers\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** The name(s) of the column(s) to check for outliers.\n- `factor`:\n  - **Type:** `number`\n  - **Description:** The IQR factor to determine the outlier threshold. Typically 1.5 for outliers or 3 for extreme outliers.\n  - **Default:** `1.5`\n- `method`:\n  - **Type:** `string`\n  - **Description:** The method to handle outliers.\n  - **Enum:** `clip` | `remove`\n  - **Default:** `clip`\n- `return_mask`:\n  - **Type:** `boolean`\n  - **Description:** If True, return a boolean mask indicating outliers instead of removing them.\n  - **Default:** `False`\n\n**Required:** `data`, `columns`  \n**Result:** Successfully detect and handle outliers in the specified column(s) of data using the IQR method  \n**Notes:**\n- This method does not assume any specific data distribution.\n- It is less sensitive to extreme outliers compared to the Z-score method.\n- May be less precise for normally distributed data compared to the Z-score method.\n- The choice of factor affects the range of what is considered an outlier.\n- Using the 'remove' method may delete data entries, which is not recommended for test sets.\n\n\n## convert_data_types\n\n**Name:** convert_data_types  \n**Description:** Convert the data type of specified columns in a DataFrame. This tool is useful for ensuring data consistency and preparing data for analysis or modeling.  \n**Applicable Situations:** data type conversion, data preprocessing, ensuring data consistency across columns\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array``\n  - **Description:** Column label or sequence of labels to convert.\n- `target_type`:\n  - **Type:** `string`\n  - **Description:** The target data type to convert to.\n  - **Enum:** `int` | `float` | `str` | `bool` | `datetime`\n\n**Required:** `data`, `columns`, `target_type`  \n**Result:** Successfully convert the data type of specified column(s) in the DataFrame  \n**Notes:**\n- For 'int' and 'float' conversions, non-numeric values will be converted to NaN.\n- The 'int' conversion uses the 'Int64' type, which supports NaN values.\n- The 'datetime' conversion will set invalid date/time values to NaT (Not a Time).\n- The 'bool' conversion may produce unexpected results for non-boolean data.\n- Consider the impact of type conversion on your data analysis and model performance.\n- Always verify the results after conversion to ensure data integrity.\n\n\n## remove_duplicates\n\n**Name:** remove_duplicates  \n**Description:** Remove duplicate rows from a DataFrame. This tool provides flexible options for identifying and handling duplicate entries in a dataset.  \n**Applicable Situations:** remove duplicate entries from a dataset, especially when dealing with data that may have been entered multiple times or when consolidating data from multiple sources\n\n**Parameters:**\n- `data`:\n  - **Type:** `pd.DataFrame`\n  - **Description:** A pandas DataFrame object representing the dataset.\n- `columns`:\n  - **Type:** ``string` | `array` | `null``\n  - **Description:** Column label or sequence of labels to consider for identifying duplicates. If None, use all columns.\n  - **Default:** `None`\n- `keep`:\n  - **Type:** `string`\n  - **Description:** Determines which duplicates (if any) to keep.\n  - **Enum:** `first` | `last` | `False`\n  - **Default:** `first`\n- `inplace`:\n  - **Type:** `boolean`\n  - **Description:** Whether to drop duplicates in place or return a copy.\n  - **Default:** `False`\n\n**Required:** `data`  \n**Result:** Successfully remove duplicate rows from the DataFrame  \n**Notes:**\n- If 'columns' is None, all columns are used for identifying duplicates.\n- The 'keep' parameter determines which duplicate rows are retained.\n- Setting 'inplace' to True will modify the original DataFrame.\n- Be cautious when removing duplicates, as it may affect the integrity of your dataset.\n- Consider the impact of removing duplicates on your analysis and model performance.\n- This method is useful for data cleaning, but make sure you understand why duplicates exist before removing them.\n\n\n"
    },
    {
        "role": "assistant",
        "content": "Thank you for providing the detailed information and specifying the constraints and tools available for the Data Cleaning phase. Below is the comprehensive solution for **Phase 3: Data Cleaning**, structured into four tasks as per your plan. Each task includes the thought process, corresponding code, and an explanation.\n\n---\n\n## TASK 1:\n**Handle Missing Values**\n\n### THOUGHT PROCESS:\nThe first step in data cleaning is to handle missing values to ensure data integrity and improve model performance. This involves:\n1. **Filling Missing Values:**\n   - **Numerical Features:** Use the median to fill missing values as it is robust against outliers.\n   - **Categorical Features:** Use the mode (most frequent value) to fill missing values.\n2. **Removing Columns with Excessive Missing Data:**\n   - Drop any column that has more than 60% missing values to reduce noise and potential biases.\n\nEnsure that both training and test datasets undergo the same processing to maintain consistency and prevent data leakage.\n\n### CODE:\n```python\nimport pandas as pd\n\n# Define the data directories\nDATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'\nCLEANED_DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'\n\n# Load the datasets\ntrain_path = DATA_DIR + 'train.csv'\ntest_path = DATA_DIR + 'test.csv'\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\n# Create copies to avoid modifying original data\ncleaned_train_df = train_df.copy()\ncleaned_test_df = test_df.copy()\n\n# Define numerical and categorical columns\nnumerical_cols = [\n    'Previous qualification (grade)', 'Admission grade',\n    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n    'Unemployment rate', 'Inflation rate', 'GDP'\n]\n\ncategorical_cols = [\n    'Marital status', 'Application mode', 'Course', 'Previous qualification',\n    'Nacionality', \"Mother's qualification\", \"Father's qualification\",\n    \"Mother's occupation\", \"Father's occupation\", 'Displaced',\n    'Educational special needs', 'Debtor', 'Tuition fees up to date',\n    'Gender', 'Scholarship holder', 'International'\n]\n\n# Task 1: Fill Missing Values\n\n# Fill missing numerical features with median\ncleaned_train_df = fill_missing_values(\n    data=cleaned_train_df,\n    columns=numerical_cols,\n    method='median'\n)\ncleaned_test_df = fill_missing_values(\n    data=cleaned_test_df,\n    columns=numerical_cols,\n    method='median'\n)\n\n# Fill missing categorical features with mode\ncleaned_train_df = fill_missing_values(\n    data=cleaned_train_df,\n    columns=categorical_cols,\n    method='mode'\n)\ncleaned_test_df = fill_missing_values(\n    data=cleaned_test_df,\n    columns=categorical_cols,\n    method='mode'\n)\n\n# Remove columns with more than 60% missing values\ncleaned_train_df = remove_columns_with_missing_data(\n    data=cleaned_train_df,\n    thresh=0.6\n)\ncleaned_test_df = remove_columns_with_missing_data(\n    data=cleaned_test_df,\n    thresh=0.6\n)\n\n# Save the cleaned datasets for verification (optional)\n# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step1.csv', index=False)\n# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step1.csv', index=False)\n```\n\n### EXPLANATION:\n- **Data Loading:** The training and test datasets are loaded from the specified directory and copied to `cleaned_train_df` and `cleaned_test_df` to prevent altering the original data.\n  \n- **Defining Columns:** Lists of numerical and categorical columns are defined based on the provided feature information.\n  \n- **Filling Missing Values:**\n  - The `fill_missing_values` tool is used to impute missing values in numerical columns with the median and in categorical columns with the mode. This ensures that the central tendency of the data is preserved without introducing significant bias.\n  \n- **Removing Columns with Excessive Missing Data:**\n  - The `remove_columns_with_missing_data` tool removes any columns that have more than 60% missing values (`thresh=0.6`). This step helps in eliminating features that may not provide reliable information for modeling.\n\n---\n\n## TASK 2:\n**Detect and Handle Outliers in Numerical Features**\n\n### THOUGHT PROCESS:\nOutliers can skew the data distribution and adversely affect model performance. Using the Interquartile Range (IQR) method:\n- **Detection:** Identify outliers that lie beyond 1.5 times the IQR from the first and third quartiles.\n- **Handling:** Apply the 'clip' method to cap the outlier values at the upper and lower bounds determined by the IQR. This approach retains the data points while minimizing the impact of extreme values.\n\nEnsure that outlier handling is applied consistently to both training and test datasets to maintain uniformity.\n\n### CODE:\n```python\n# Task 2: Detect and Handle Outliers in Numerical Features\n\n# Define the numerical columns again in case some were removed in Task 1\nnumerical_cols = [\n    'Previous qualification (grade)', 'Admission grade',\n    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n    'Unemployment rate', 'Inflation rate', 'GDP'\n]\n\n# Handle possible removal of some numerical columns in Task 1\nnumerical_cols = [col for col in numerical_cols if col in cleaned_train_df.columns]\n\n# Detect and handle outliers by clipping\ncleaned_train_df = detect_and_handle_outliers_iqr(\n    data=cleaned_train_df,\n    columns=numerical_cols,\n    factor=1.5,\n    method='clip'\n)\n\ncleaned_test_df = detect_and_handle_outliers_iqr(\n    data=cleaned_test_df,\n    columns=numerical_cols,\n    factor=1.5,\n    method='clip'\n)\n\n# Save the cleaned datasets for verification (optional)\n# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step2.csv', index=False)\n# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step2.csv', index=False)\n```\n\n### EXPLANATION:\n- **Defining Numerical Columns:** After removing columns with excessive missing data in Task 1, some numerical columns might have been dropped. Therefore, the list `numerical_cols` is updated to include only those columns that still exist in the cleaned datasets.\n  \n- **Outlier Detection and Handling:**\n  - The `detect_and_handle_outliers_iqr` tool is applied with `factor=1.5` and `method='clip'`. This caps the outlier values at the calculated lower and upper bounds based on the IQR, effectively reducing the impact of extreme values while retaining all data points.\n  \n- **Consistency:** The same processing is applied to both training and test datasets to maintain consistency and avoid discrepancies during model training and prediction.\n\n---\n\n## TASK 3:\n**Normalize and Standardize Categorical Features**\n\n### THOUGHT PROCESS:\nStandardizing categorical features ensures consistency in data formatting, which is crucial for accurate modeling and analysis. The steps include:\n1. **Data Type Conversion:** Convert all categorical columns to string type using the `convert_data_types` tool.\n2. **String Normalization:**\n   - Convert all text to lowercase to ensure uniformity.\n   - Strip leading and trailing whitespaces to remove unintended spaces.\n   - Correct common typos based on predefined mappings or manual inspection (requires predefined mappings; assuming a placeholder for this step).\n\nMaintaining the semantic integrity of categorical variables is essential to prevent merging distinct categories inadvertently.\n\n### CODE:\n```python\n# Task 3: Normalize and Standardize Categorical Features\n\n# Define categorical columns again in case some were removed in Task 1\ncategorical_cols = [\n    'Marital status', 'Application mode', 'Course', 'Previous qualification',\n    'Nacionality', \"Mother's qualification\", \"Father's qualification\",\n    \"Mother's occupation\", \"Father's occupation\", 'Displaced',\n    'Educational special needs', 'Debtor', 'Tuition fees up to date',\n    'Gender', 'Scholarship holder', 'International'\n]\n\n# Handle possible removal of some categorical columns in Task 1\ncategorical_cols = [col for col in categorical_cols if col in cleaned_train_df.columns]\n\n# Convert categorical columns to string type\ncleaned_train_df = convert_data_types(\n    data=cleaned_train_df,\n    columns=categorical_cols,\n    target_type='str'\n)\n\ncleaned_test_df = convert_data_types(\n    data=cleaned_test_df,\n    columns=categorical_cols,\n    target_type='str'\n)\n\n# Normalize categorical strings: lowercase and strip whitespaces\nfor col in categorical_cols:\n    cleaned_train_df[col] = cleaned_train_df[col].str.lower().str.strip()\n    cleaned_test_df[col] = cleaned_test_df[col].str.lower().str.strip()\n\n# Placeholder for correcting common typos\n# Define a dictionary for typo corrections if available\n# Example:\n# typo_mapping = {\n#     'marial status': 'marital status',\n#     'scholarship holder': 'scholarship_holder',\n#     # Add more mappings as identified\n# }\n\n# Uncomment and modify the following lines if typo mappings are available\n# for col, mapping in typo_mapping.items():\n#     cleaned_train_df[col] = cleaned_train_df[col].replace(mapping)\n#     cleaned_test_df[col] = cleaned_test_df[col].replace(mapping)\n\n# Save the cleaned datasets for verification (optional)\n# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step3.csv', index=False)\n# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step3.csv', index=False)\n```\n\n### EXPLANATION:\n- **Defining Categorical Columns:** Similar to numerical columns, categorical columns might have been removed in Task 1. The list `categorical_cols` is updated accordingly.\n  \n- **Data Type Conversion:** The `convert_data_types` tool ensures that all categorical columns are of string type (`str`), which is essential for string operations.\n  \n- **String Normalization:**\n  - **Lowercasing:** Converts all text to lowercase to eliminate case sensitivity issues.\n  - **Stripping Whitespaces:** Removes any leading or trailing spaces that might cause discrepancies.\n  \n- **Typo Correction:** While the plan mentions correcting common typos, specific mappings are required. A placeholder is provided where predefined typo mappings can be applied. This step ensures that similar categories with typos are standardized, preventing the merging of distinct categories unintentionally.\n\n---\n\n## TASK 4:\n**Remove Duplicate Rows and Convert Data Types**\n\n### THOUGHT PROCESS:\nEnsuring the uniqueness of each record is vital to maintain data quality. This task involves:\n1. **Removing Duplicates:**\n   - Identify and remove duplicate rows based on all columns except the `id` to preserve the unique identifier.\n   - Retain the first occurrence of each duplicate.\n2. **Converting Data Types:**\n   - **Numerical Columns:** Ensure they are of type `int` or `float` as appropriate.\n   - **Categorical Columns:** Ensure they remain as `str`.\n   - **ID Column:** Convert to `str` to preserve formatting and prevent issues like the removal of leading zeros.\n\nThese steps enhance data integrity and prepare the dataset for subsequent analysis and modeling.\n\n### CODE:\n```python\n# Task 4: Remove Duplicate Rows and Convert Data Types\n\n# Remove duplicates based on all columns except 'id'\ncolumns_to_consider = [col for col in cleaned_train_df.columns if col != 'id']\ncleaned_train_df = remove_duplicates(\n    data=cleaned_train_df,\n    columns=columns_to_consider,\n    keep='first'\n)\n\ncleaned_test_df = remove_duplicates(\n    data=cleaned_test_df,\n    columns=columns_to_consider,\n    keep='first'\n)\n\n# Define numerical columns again after Task 1 and Task 2\nnumerical_cols = [\n    'Previous qualification (grade)', 'Admission grade',\n    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n    'Unemployment rate', 'Inflation rate', 'GDP', 'Age at enrollment', 'Application order'\n]\n\n# Ensure numerical columns exist after previous tasks\nnumerical_cols = [col for col in numerical_cols if col in cleaned_train_df.columns]\n\n# Convert numerical columns to appropriate data types\ncleaned_train_df = convert_data_types(\n    data=cleaned_train_df,\n    columns=numerical_cols,\n    target_type='float'  # Assuming all are floats; adjust if some are integers\n)\n\ncleaned_test_df = convert_data_types(\n    data=cleaned_test_df,\n    columns=numerical_cols,\n    target_type='float'\n)\n\n# Convert 'id' column to string\ncleaned_train_df = convert_data_types(\n    data=cleaned_train_df,\n    columns=['id'],\n    target_type='str'\n)\n\ncleaned_test_df = convert_data_types(\n    data=cleaned_test_df,\n    columns=['id'],\n    target_type='str'\n)\n\n# Verify data types (optional)\n# print(cleaned_train_df.dtypes)\n# print(cleaned_test_df.dtypes)\n\n# Save the final cleaned datasets\ncleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train.csv', index=False)\ncleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test.csv', index=False)\n\nprint(\"Data Cleaning Phase Completed: 'cleaned_train.csv' and 'cleaned_test.csv' have been saved successfully.\")\n```\n\n### EXPLANATION:\n- **Removing Duplicates:**\n  - The `remove_duplicates` tool is utilized to drop duplicate rows based on all columns except the `id`. This ensures that each `id` remains unique while eliminating redundant data entries.\n  - The parameter `keep='first'` retains the first occurrence of each duplicate, ensuring that no unique data is unnecessarily discarded.\n  \n- **Data Type Conversion:**\n  - **Numerical Columns:** All numerical columns are converted to `float` type using the `convert_data_types` tool. If certain columns are strictly integers, adjust `target_type` accordingly for those specific columns.\n  - **ID Column:** The `id` column is converted to `str` to preserve its formatting, preventing issues like the removal of leading zeros which could alter the identifier's integrity.\n  \n- **Saving Cleaned Data:**\n  - The final cleaned datasets, `cleaned_train.csv` and `cleaned_test.csv`, are saved in the specified directory. This aligns with the Data Saving constraints provided.\n  \n- **Completion Message:** A print statement confirms the successful completion of the Data Cleaning phase and the saving of the cleaned datasets.\n\n---\n\n## FINAL OUTPUT:\nAfter executing all four tasks, the cleaned datasets `cleaned_train.csv` and `cleaned_test.csv` are available in the specified directory:\n```\n/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/\n```\n\nThese datasets have:\n- Handled missing values appropriately.\n- Removed columns with excessive missing data.\n- Managed outliers in numerical features.\n- Standardized categorical features for consistency.\n- Removed duplicate rows and ensured correct data types.\n\nThis completes the Data Cleaning phase, setting a strong foundation for the subsequent phases of Exploratory Data Analysis, Feature Engineering, and Model Building."
    }
]