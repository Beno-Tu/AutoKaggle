Thank you for providing the detailed information and specifying the constraints and tools available for the Data Cleaning phase. Below is the comprehensive solution for **Phase 3: Data Cleaning**, structured into four tasks as per your plan. Each task includes the thought process, corresponding code, and an explanation.

---

## TASK 1:
**Handle Missing Values**

### THOUGHT PROCESS:
The first step in data cleaning is to handle missing values to ensure data integrity and improve model performance. This involves:
1. **Filling Missing Values:**
   - **Numerical Features:** Use the median to fill missing values as it is robust against outliers.
   - **Categorical Features:** Use the mode (most frequent value) to fill missing values.
2. **Removing Columns with Excessive Missing Data:**
   - Drop any column that has more than 60% missing values to reduce noise and potential biases.

Ensure that both training and test datasets undergo the same processing to maintain consistency and prevent data leakage.

### CODE:
```python
import pandas as pd

# Define the data directories
DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'
CLEANED_DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'

# Load the datasets
train_path = DATA_DIR + 'train.csv'
test_path = DATA_DIR + 'test.csv'

train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

# Create copies to avoid modifying original data
cleaned_train_df = train_df.copy()
cleaned_test_df = test_df.copy()

# Define numerical and categorical columns
numerical_cols = [
    'Previous qualification (grade)', 'Admission grade',
    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate', 'Inflation rate', 'GDP'
]

categorical_cols = [
    'Marital status', 'Application mode', 'Course', 'Previous qualification',
    'Nacionality', "Mother's qualification", "Father's qualification",
    "Mother's occupation", "Father's occupation", 'Displaced',
    'Educational special needs', 'Debtor', 'Tuition fees up to date',
    'Gender', 'Scholarship holder', 'International'
]

# Task 1: Fill Missing Values

# Fill missing numerical features with median
cleaned_train_df = fill_missing_values(
    data=cleaned_train_df,
    columns=numerical_cols,
    method='median'
)
cleaned_test_df = fill_missing_values(
    data=cleaned_test_df,
    columns=numerical_cols,
    method='median'
)

# Fill missing categorical features with mode
cleaned_train_df = fill_missing_values(
    data=cleaned_train_df,
    columns=categorical_cols,
    method='mode'
)
cleaned_test_df = fill_missing_values(
    data=cleaned_test_df,
    columns=categorical_cols,
    method='mode'
)

# Remove columns with more than 60% missing values
cleaned_train_df = remove_columns_with_missing_data(
    data=cleaned_train_df,
    thresh=0.6
)
cleaned_test_df = remove_columns_with_missing_data(
    data=cleaned_test_df,
    thresh=0.6
)

# Save the cleaned datasets for verification (optional)
# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step1.csv', index=False)
# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step1.csv', index=False)
```

### EXPLANATION:
- **Data Loading:** The training and test datasets are loaded from the specified directory and copied to `cleaned_train_df` and `cleaned_test_df` to prevent altering the original data.
  
- **Defining Columns:** Lists of numerical and categorical columns are defined based on the provided feature information.
  
- **Filling Missing Values:**
  - The `fill_missing_values` tool is used to impute missing values in numerical columns with the median and in categorical columns with the mode. This ensures that the central tendency of the data is preserved without introducing significant bias.
  
- **Removing Columns with Excessive Missing Data:**
  - The `remove_columns_with_missing_data` tool removes any columns that have more than 60% missing values (`thresh=0.6`). This step helps in eliminating features that may not provide reliable information for modeling.

---

## TASK 2:
**Detect and Handle Outliers in Numerical Features**

### THOUGHT PROCESS:
Outliers can skew the data distribution and adversely affect model performance. Using the Interquartile Range (IQR) method:
- **Detection:** Identify outliers that lie beyond 1.5 times the IQR from the first and third quartiles.
- **Handling:** Apply the 'clip' method to cap the outlier values at the upper and lower bounds determined by the IQR. This approach retains the data points while minimizing the impact of extreme values.

Ensure that outlier handling is applied consistently to both training and test datasets to maintain uniformity.

### CODE:
```python
# Task 2: Detect and Handle Outliers in Numerical Features

# Define the numerical columns again in case some were removed in Task 1
numerical_cols = [
    'Previous qualification (grade)', 'Admission grade',
    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate', 'Inflation rate', 'GDP'
]

# Handle possible removal of some numerical columns in Task 1
numerical_cols = [col for col in numerical_cols if col in cleaned_train_df.columns]

# Detect and handle outliers by clipping
cleaned_train_df = detect_and_handle_outliers_iqr(
    data=cleaned_train_df,
    columns=numerical_cols,
    factor=1.5,
    method='clip'
)

cleaned_test_df = detect_and_handle_outliers_iqr(
    data=cleaned_test_df,
    columns=numerical_cols,
    factor=1.5,
    method='clip'
)

# Save the cleaned datasets for verification (optional)
# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step2.csv', index=False)
# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step2.csv', index=False)
```

### EXPLANATION:
- **Defining Numerical Columns:** After removing columns with excessive missing data in Task 1, some numerical columns might have been dropped. Therefore, the list `numerical_cols` is updated to include only those columns that still exist in the cleaned datasets.
  
- **Outlier Detection and Handling:**
  - The `detect_and_handle_outliers_iqr` tool is applied with `factor=1.5` and `method='clip'`. This caps the outlier values at the calculated lower and upper bounds based on the IQR, effectively reducing the impact of extreme values while retaining all data points.
  
- **Consistency:** The same processing is applied to both training and test datasets to maintain consistency and avoid discrepancies during model training and prediction.

---

## TASK 3:
**Normalize and Standardize Categorical Features**

### THOUGHT PROCESS:
Standardizing categorical features ensures consistency in data formatting, which is crucial for accurate modeling and analysis. The steps include:
1. **Data Type Conversion:** Convert all categorical columns to string type using the `convert_data_types` tool.
2. **String Normalization:**
   - Convert all text to lowercase to ensure uniformity.
   - Strip leading and trailing whitespaces to remove unintended spaces.
   - Correct common typos based on predefined mappings or manual inspection (requires predefined mappings; assuming a placeholder for this step).

Maintaining the semantic integrity of categorical variables is essential to prevent merging distinct categories inadvertently.

### CODE:
```python
# Task 3: Normalize and Standardize Categorical Features

# Define categorical columns again in case some were removed in Task 1
categorical_cols = [
    'Marital status', 'Application mode', 'Course', 'Previous qualification',
    'Nacionality', "Mother's qualification", "Father's qualification",
    "Mother's occupation", "Father's occupation", 'Displaced',
    'Educational special needs', 'Debtor', 'Tuition fees up to date',
    'Gender', 'Scholarship holder', 'International'
]

# Handle possible removal of some categorical columns in Task 1
categorical_cols = [col for col in categorical_cols if col in cleaned_train_df.columns]

# Convert categorical columns to string type
cleaned_train_df = convert_data_types(
    data=cleaned_train_df,
    columns=categorical_cols,
    target_type='str'
)

cleaned_test_df = convert_data_types(
    data=cleaned_test_df,
    columns=categorical_cols,
    target_type='str'
)

# Normalize categorical strings: lowercase and strip whitespaces
for col in categorical_cols:
    cleaned_train_df[col] = cleaned_train_df[col].str.lower().str.strip()
    cleaned_test_df[col] = cleaned_test_df[col].str.lower().str.strip()

# Placeholder for correcting common typos
# Define a dictionary for typo corrections if available
# Example:
# typo_mapping = {
#     'marial status': 'marital status',
#     'scholarship holder': 'scholarship_holder',
#     # Add more mappings as identified
# }

# Uncomment and modify the following lines if typo mappings are available
# for col, mapping in typo_mapping.items():
#     cleaned_train_df[col] = cleaned_train_df[col].replace(mapping)
#     cleaned_test_df[col] = cleaned_test_df[col].replace(mapping)

# Save the cleaned datasets for verification (optional)
# cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train_step3.csv', index=False)
# cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test_step3.csv', index=False)
```

### EXPLANATION:
- **Defining Categorical Columns:** Similar to numerical columns, categorical columns might have been removed in Task 1. The list `categorical_cols` is updated accordingly.
  
- **Data Type Conversion:** The `convert_data_types` tool ensures that all categorical columns are of string type (`str`), which is essential for string operations.
  
- **String Normalization:**
  - **Lowercasing:** Converts all text to lowercase to eliminate case sensitivity issues.
  - **Stripping Whitespaces:** Removes any leading or trailing spaces that might cause discrepancies.
  
- **Typo Correction:** While the plan mentions correcting common typos, specific mappings are required. A placeholder is provided where predefined typo mappings can be applied. This step ensures that similar categories with typos are standardized, preventing the merging of distinct categories unintentionally.

---

## TASK 4:
**Remove Duplicate Rows and Convert Data Types**

### THOUGHT PROCESS:
Ensuring the uniqueness of each record is vital to maintain data quality. This task involves:
1. **Removing Duplicates:**
   - Identify and remove duplicate rows based on all columns except the `id` to preserve the unique identifier.
   - Retain the first occurrence of each duplicate.
2. **Converting Data Types:**
   - **Numerical Columns:** Ensure they are of type `int` or `float` as appropriate.
   - **Categorical Columns:** Ensure they remain as `str`.
   - **ID Column:** Convert to `str` to preserve formatting and prevent issues like the removal of leading zeros.

These steps enhance data integrity and prepare the dataset for subsequent analysis and modeling.

### CODE:
```python
# Task 4: Remove Duplicate Rows and Convert Data Types

# Remove duplicates based on all columns except 'id'
columns_to_consider = [col for col in cleaned_train_df.columns if col != 'id']
cleaned_train_df = remove_duplicates(
    data=cleaned_train_df,
    columns=columns_to_consider,
    keep='first'
)

cleaned_test_df = remove_duplicates(
    data=cleaned_test_df,
    columns=columns_to_consider,
    keep='first'
)

# Define numerical columns again after Task 1 and Task 2
numerical_cols = [
    'Previous qualification (grade)', 'Admission grade',
    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate', 'Inflation rate', 'GDP', 'Age at enrollment', 'Application order'
]

# Ensure numerical columns exist after previous tasks
numerical_cols = [col for col in numerical_cols if col in cleaned_train_df.columns]

# Convert numerical columns to appropriate data types
cleaned_train_df = convert_data_types(
    data=cleaned_train_df,
    columns=numerical_cols,
    target_type='float'  # Assuming all are floats; adjust if some are integers
)

cleaned_test_df = convert_data_types(
    data=cleaned_test_df,
    columns=numerical_cols,
    target_type='float'
)

# Convert 'id' column to string
cleaned_train_df = convert_data_types(
    data=cleaned_train_df,
    columns=['id'],
    target_type='str'
)

cleaned_test_df = convert_data_types(
    data=cleaned_test_df,
    columns=['id'],
    target_type='str'
)

# Verify data types (optional)
# print(cleaned_train_df.dtypes)
# print(cleaned_test_df.dtypes)

# Save the final cleaned datasets
cleaned_train_df.to_csv(CLEANED_DATA_DIR + 'cleaned_train.csv', index=False)
cleaned_test_df.to_csv(CLEANED_DATA_DIR + 'cleaned_test.csv', index=False)

print("Data Cleaning Phase Completed: 'cleaned_train.csv' and 'cleaned_test.csv' have been saved successfully.")
```

### EXPLANATION:
- **Removing Duplicates:**
  - The `remove_duplicates` tool is utilized to drop duplicate rows based on all columns except the `id`. This ensures that each `id` remains unique while eliminating redundant data entries.
  - The parameter `keep='first'` retains the first occurrence of each duplicate, ensuring that no unique data is unnecessarily discarded.
  
- **Data Type Conversion:**
  - **Numerical Columns:** All numerical columns are converted to `float` type using the `convert_data_types` tool. If certain columns are strictly integers, adjust `target_type` accordingly for those specific columns.
  - **ID Column:** The `id` column is converted to `str` to preserve its formatting, preventing issues like the removal of leading zeros which could alter the identifier's integrity.
  
- **Saving Cleaned Data:**
  - The final cleaned datasets, `cleaned_train.csv` and `cleaned_test.csv`, are saved in the specified directory. This aligns with the Data Saving constraints provided.
  
- **Completion Message:** A print statement confirms the successful completion of the Data Cleaning phase and the saving of the cleaned datasets.

---

## FINAL OUTPUT:
After executing all four tasks, the cleaned datasets `cleaned_train.csv` and `cleaned_test.csv` are available in the specified directory:
```
/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/
```

These datasets have:
- Handled missing values appropriately.
- Removed columns with excessive missing data.
- Managed outliers in numerical features.
- Standardized categorical features for consistency.
- Removed duplicate rows and ensured correct data types.

This completes the Data Cleaning phase, setting a strong foundation for the subsequent phases of Exploratory Data Analysis, Feature Engineering, and Model Building.