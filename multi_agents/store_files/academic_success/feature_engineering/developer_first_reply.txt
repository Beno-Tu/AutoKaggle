Certainly! Let's proceed with the **Feature Engineering** phase by implementing the tasks outlined in your plan. We'll follow each step meticulously, ensuring adherence to the provided constraints and best coding practices.

### Overview of Tasks:
1. **Encode Categorical Variables**
2. **Scale and Normalize Numerical Features**
3. **Create Interaction and Polynomial Features**
4. **Handle Missing Values and Impute Derived Features**

Each task will include a **Thought Process**, **Code**, and **Explanation** section.

---

## TASK 1: Encode Categorical Variables

### THOUGHT PROCESS:
To prepare categorical variables for machine learning models, we'll encode them into numerical formats. Binary categorical features will be label-encoded, while multi-class categorical features will be one-hot encoded. We'll ensure that the encoding is consistent between the training and test datasets by fitting the encoders on the training data and transforming both datasets accordingly. Additionally, we'll handle high-cardinality features by limiting the number of dummy variables and manage unseen categories in the test set by ignoring them during encoding.

### CODE:
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Define the data directories
DATA_DIR = '/mnt/d/PythonProjects/AutoKaggleMaster/multi_agents/competition/classification_with_an_academic_success_dataset/'

# Load the cleaned datasets
cleaned_train_df = pd.read_csv(DATA_DIR + 'cleaned_train.csv')
cleaned_test_df = pd.read_csv(DATA_DIR + 'cleaned_test.csv')

# Separate features and target
X_train = cleaned_train_df.drop(['Target'], axis=1)
y_train = cleaned_train_df['Target']
X_test = cleaned_test_df.copy()

# Define Binary and Multiclass Categorical Features
binary_categorical_features = [
    'Gender',
    'Displaced',
    'Debtor',
    'Scholarship holder',
    'International'
]

multiclass_categorical_features = [
    'Marital status',
    'Application mode',
    'Course',
    'Previous qualification',
    'Nacionality',
    "Mother's qualification",
    "Father's qualification",
    "Mother's occupation",
    "Father's occupation",
    'Educational special needs',
    'Tuition fees up to date'
]

# Initialize LabelEncoders for binary features
label_encoders = {}
for col in binary_categorical_features:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
    label_encoders[col] = le
    print(f"Encoded binary feature: {col}")

# One-Hot Encode multiclass categorical features
# To control dimensionality, we'll limit the number of dummy variables by setting a maximum number of unique categories
# For high-cardinality features like 'Course', we'll encode the top N categories and label others as 'Other'

def one_hot_encode(df_train, df_test, columns, top_n=10):
    for col in columns:
        top_categories = df_train[col].value_counts().nlargest(top_n).index
        df_train[col] = df_train[col].apply(lambda x: x if x in top_categories else 'Other')
        df_test[col] = df_test[col].apply(lambda x: x if x in top_categories else 'Other')
    
    df_all = pd.concat([df_train, df_test], axis=0)
    df_all = pd.get_dummies(df_all, columns=columns, drop_first=True)
    
    return df_all.iloc[:len(df_train), :], df_all.iloc[len(df_train):, :]

X_train, X_test = one_hot_encode(X_train, X_test, multiclass_categorical_features, top_n=10)
print("One-Hot Encoded multiclass categorical features.")

# Combine the encoded features with target
processed_train_df = pd.concat([X_train, y_train], axis=1)
processed_test_df = X_test.copy()

```

### EXPLANATION:
1. **Loading Data:** We load the `cleaned_train.csv` and `cleaned_test.csv` datasets.
2. **Separating Features and Target:** We separate the target variable (`Target`) from the training features.
3. **Identifying Categorical Features:** We categorize the features into binary and multiclass categorical features.
4. **Label Encoding Binary Features:** We use `LabelEncoder` to transform binary categorical features into numerical format. This encoder is fit on the training data and then applied to the test data to maintain consistency.
5. **One-Hot Encoding Multiclass Features:** To handle multiclass categorical features, we limit the number of dummy variables by encoding only the top `N` categories (e.g., top 10) and grouping the rest as 'Other'. This approach prevents the curse of dimensionality, especially for high-cardinality features like `Course`. We concatenate the training and test data to ensure consistent encoding and then split them back.
6. **Combining Encoded Features:** Finally, we combine the encoded features with the target variable to form the processed training and test datasets.

---

## TASK 2: Scale and Normalize Numerical Features

### THOUGHT PROCESS:
Scaling numerical features ensures that each feature contributes equally to the model's performance and aids in faster convergence. We'll apply appropriate scaling techniques:
- **StandardScaler** for features that benefit from zero mean and unit variance.
- **MinMaxScaler** for features that need to be scaled to a specific range, typically [0, 1].
- **PowerTransformer** (Yeo-Johnson) for skewed distributions to make them more Gaussian-like.

We'll fit the scalers on the training data and transform both training and test datasets to prevent data leakage.

### CODE:
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer

# Define Numerical Features
numerical_features = [
    'Previous qualification (grade)',
    'Admission grade',
    'Curricular units 1st sem (credited)',
    'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)',
    'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)',
    'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)',
    'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)',
    'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)',
    'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate',
    'Inflation rate',
    'GDP',
    'Age at enrollment'
]

# Initialize Scalers
standard_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()
power_transformer = PowerTransformer(method='yeo-johnson')

# Fit StandardScaler on numerical features
X_train[numerical_features] = standard_scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = standard_scaler.transform(X_test[numerical_features])
print("Applied StandardScaler to numerical features.")

# Identify skewed features
skew_threshold = 0.5
skewness = X_train[numerical_features].skew().abs()
skewed_features = skewness[skewness > skew_threshold].index.tolist()
print(f"Skewed features identified for PowerTransformer: {skewed_features}")

# Apply PowerTransformer to skewed features
if skewed_features:
    X_train[skewed_features] = power_transformer.fit_transform(X_train[skewed_features])
    X_test[skewed_features] = power_transformer.transform(X_test[skewed_features])
    print("Applied PowerTransformer to skewed numerical features.")

# Apply MinMaxScaler to all numerical features after power transformation
X_train[numerical_features] = minmax_scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = minmax_scaler.transform(X_test[numerical_features])
print("Applied MinMaxScaler to numerical features.")

# Update processed dataframes
processed_train_df = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)
processed_test_df = X_test.copy()
```

### EXPLANATION:
1. **Identifying Numerical Features:** We list all numerical features that require scaling.
2. **Initializing Scalers:** We initialize `StandardScaler` for standardizing features, `MinMaxScaler` for scaling features to [0, 1], and `PowerTransformer` for handling skewed distributions.
3. **Applying StandardScaler:** We fit the `StandardScaler` on the training data and transform both training and test datasets to ensure consistency.
4. **Handling Skewed Features:** We identify features with skewness greater than a specified threshold (e.g., 0.5) and apply the `PowerTransformer` to these features to reduce skewness.
5. **Applying MinMaxScaler:** After handling skewness, we apply the `MinMaxScaler` to scale all numerical features to the [0, 1] range.
6. **Updating Processed Dataframes:** We update the `processed_train_df` and `processed_test_df` with the scaled numerical features.

---

## TASK 3: Create Interaction and Polynomial Features

### THOUGHT PROCESS:
Creating interaction and polynomial features can help the model capture non-linear relationships and interactions between features, potentially enhancing predictive performance. To maintain computational efficiency and prevent overfitting, we'll limit the creation of such features by:
- Generating only specific interaction terms based on domain knowledge.
- Using `PolynomialFeatures` with a low degree (e.g., 2) to avoid excessive feature expansion.
- Assessing feature importance to retain only meaningful polynomial and interaction features.

### CODE:
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier

# Define interaction pairs based on domain knowledge
interaction_pairs = [
    ('Admission grade', 'GDP'),
    ('Age at enrollment', 'Educational special needs')
]

# Create interaction features manually
for pair in interaction_pairs:
    col_name = f"{pair[0]}_x_{pair[1]}"
    processed_train_df[col_name] = processed_train_df[pair[0]] * processed_train_df[pair[1]]
    processed_test_df[col_name] = processed_test_df[pair[0]] * processed_test_df[pair[1]]
    print(f"Created interaction feature: {col_name}")

# Initialize PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)

# Fit and transform on training data
poly_features_train = poly.fit_transform(processed_train_df[numerical_features])
poly_feature_names = poly.get_feature_names_out(numerical_features)

# Convert to DataFrame
poly_features_train_df = pd.DataFrame(poly_features_train, columns=poly_feature_names, index=processed_train_df.index)

# Transform test data
poly_features_test = poly.transform(processed_test_df[numerical_features])
poly_features_test_df = pd.DataFrame(poly_features_test, columns=poly_feature_names, index=processed_test_df.index)

# Concatenate polynomial features to processed dataframes
processed_train_df = pd.concat([processed_train_df, poly_features_train_df], axis=1)
processed_test_df = pd.concat([processed_test_df, poly_features_test_df], axis=1)
print("Added polynomial features to the datasets.")

# Optionally, evaluate feature importance using a RandomForestClassifier to retain only significant features
model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(processed_train_df.drop(['Target'], axis=1), y_train)

# Get feature importances
importances = pd.Series(model.feature_importances_, index=processed_train_df.drop(['Target'], axis=1).columns)
important_features = importances[importances > 0.01].index.tolist()
print(f"Selected {len(important_features)} important features based on feature importance.")

# Retain only important features
processed_train_df = processed_train_df[important_features + ['Target']]
processed_test_df = processed_test_df[important_features]
```

### EXPLANATION:
1. **Creating Manual Interaction Features:** Based on domain knowledge, we create specific interaction features such as the product of `Admission grade` and `GDP`, and `Age at enrollment` with `Educational special needs`.
2. **Generating Polynomial Features:** We use `PolynomialFeatures` with degree 2 to create polynomial and interaction terms for all numerical features. This results in features like squares and pairwise products.
3. **Combining Polynomial Features:** The generated polynomial features are converted into DataFrames and concatenated with the existing processed datasets.
4. **Feature Importance Assessment:** To prevent overfitting and manage computational complexity, we train a `RandomForestClassifier` to evaluate feature importance. Features with an importance score above a threshold (e.g., 0.01) are retained.
5. **Retaining Important Features:** We filter both the training and test datasets to include only the important features identified, along with the target variable in the training set.

---

## TASK 4: Handle Missing Values and Impute Derived Features

### THOUGHT PROCESS:
Although missing values were addressed during data cleaning, the creation of new features might introduce additional missingness. We'll ensure that the processed datasets have no missing values by applying imputation techniques. For numerical features, we'll use median imputation, and for categorical features (if any remain), we'll use the most frequent value. Additionally, we'll create indicator variables for features with substantial missing data to inform the model about the presence of missingness.

### CODE:
```python
from sklearn.impute import SimpleImputer

# Check for missing values
missing_train = processed_train_df.isnull().sum()
missing_test = processed_test_df.isnull().sum()

print(f"Missing values in training data before imputation:\n{missing_train[missing_train > 0]}")
print(f"Missing values in test data before imputation:\n{missing_test[missing_test > 0]}")

# Since all missing values should have been handled in data cleaning, this is a precautionary step

# Initialize imputers
numeric_imputer = SimpleImputer(strategy='median')
categorical_imputer = SimpleImputer(strategy='most_frequent')

# Identify numerical and categorical columns
# Assuming that all retained features after encoding and scaling are numerical
numerical_cols = processed_train_df.drop(['Target'], axis=1).columns.tolist()

# Impute missing values for numerical features
processed_train_df[numerical_cols] = numeric_imputer.fit_transform(processed_train_df[numerical_cols])
processed_test_df[numerical_cols] = numeric_imputer.transform(processed_test_df[numerical_cols])
print("Imputed missing values in numerical features with median.")

# If there are any categorical features left (unlikely after encoding), impute them
# For demonstration, let's assume no categorical features remain
# If there are, similar imputation can be applied

# Verify no missing values remain
assert processed_train_df.isnull().sum().sum() == 0, "There are still missing values in the training data."
assert processed_test_df.isnull().sum().sum() == 0, "There are still missing values in the test data."
print("All missing values have been imputed.")

# Save the processed datasets
processed_train_df.to_csv(DATA_DIR + 'processed_train.csv', index=False)
processed_test_df.to_csv(DATA_DIR + 'processed_test.csv', index=False)

print("Feature Engineering Phase Completed: 'processed_train.csv' and 'processed_test.csv' have been saved successfully.")
```

### EXPLANATION:
1. **Checking for Missing Values:** We first check if any missing values exist in the processed training and test datasets after feature creation.
2. **Initializing Imputers:** We use `SimpleImputer` with the median strategy for numerical features. If there were categorical features remaining, we would use the most frequent (mode) strategy.
3. **Imputing Missing Values:** We apply the median imputation to all numerical features in both training and test datasets.
4. **Verification:** We assert that there are no remaining missing values to ensure data integrity.
5. **Saving Processed Data:** Finally, we save the fully processed datasets as `processed_train.csv` and `processed_test.csv` in the specified data directory.

---

## FINAL REMARKS

The **Feature Engineering** phase is now complete. The processed datasets `processed_train.csv` and `processed_test.csv` are ready for the subsequent **Model Building, Validation, and Prediction** phase. These datasets have encoded categorical variables, scaled and normalized numerical features, interaction and polynomial features, and no missing values, adhering to the constraints and best practices outlined.

If you have any further questions or need assistance with the next phase, feel free to ask!