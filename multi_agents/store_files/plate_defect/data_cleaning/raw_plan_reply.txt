Let's break down the Data Cleaning phase into **four tasks** focusing on the key areas identified from the previous phase report. This plan will be specific to each feature and will utilize the available tools efficiently.

### Task 1: Handling Missing Values

**Objective:** Address missing values in both `train.csv` and `test.csv` using appropriate techniques.

**Essential Actions:**
1. **Identify Missing Values:**
   - **Tool:** Pandas (`data.isnull().sum()`)
   - **Parameters:** None
   - **Expected Output:** Print the count of missing values for each feature.

2. **Determine Missing Value Strategy:**
   - For numerical features:
     - Use mean/median/mode based on distribution and domain knowledge.
   - For categorical features:
     - Use mode or a constant value if mode is not appropriate.
   - **Tool:** `fill_missing_values`
   - **Parameters:** 
     - `method`: 'auto' or specific methods like 'mean', 'median', 'mode', 'constant'
     - `fill_value`: if using 'constant', specify the value.

**Features Involved:**
- Numerical: All specified numerical features.
- Categorical: `TypeOfSteel_A300`, `TypeOfSteel_A400`

**Constraints:** Ensure consistency in handling missing values across both datasets. 

**Example:**
```python
# Numerical Features
fill_missing_values(data=train, columns=['X_Minimum', 'X_Maximum', ...], method='mean')
fill_missing_values(data=test, columns=['X_Minimum', 'X_Maximum', ...], method='mean')

# Categorical Features
fill_missing_values(data=train, columns=['TypeOfSteel_A300', 'TypeOfSteel_A400'], method='mode')
fill_missing_values(data=test, columns=['TypeOfSteel_A300', 'TypeOfSteel_A400'], method='mode')
```

### Task 2: Treating Outliers

**Objective:** Detect and handle outliers in the numerical features to ensure data quality.

**Essential Actions:**
1. **Detect Outliers:**
   - **Tool:** `detect_and_handle_outliers_iqr` and `detect_and_handle_outliers_zscore`
   - **Parameters:** 
     - `columns`: List of numerical columns to check.
     - `factor` or `threshold`: Typically 1.5 for IQR and 3.0 for Z-score.
     - `method`: 'clip' to cap outliers or 'remove' to delete them.
   - **Expected Output:** Print summary of outliers detected and handled.

**Features Involved:**
- Numerical: `Steel_Plate_Thickness`, `Maximum_of_Luminosity`, `Minimum_of_Luminosity`, and other relevant features.

**Constraints:** Handle outliers consistently across both datasets without removing data entries from the test set.

**Example:**
```python
# IQR Method
detect_and_handle_outliers_iqr(data=train, columns=['Steel_Plate_Thickness', 'Maximum_of_Luminosity', ...], factor=1.5, method='clip')
detect_and_handle_outliers_iqr(data=test, columns=['Steel_Plate_Thickness', 'Maximum_of_Luminosity', ...], factor=1.5, method='clip')
```

### Task 3: Ensuring Data Consistency

**Objective:** Ensure consistency in data types and remove duplicate rows to maintain data integrity.

**Essential Actions:**
1. **Convert Data Types:**
   - **Tool:** `convert_data_types`
   - **Parameters:** 
     - `columns`: List of columns to convert.
     - `target_type`: Desired data type (e.g., `int`, `float`, `bool`).
   - **Expected Output:** Print confirmation of data type conversions.

2. **Remove Duplicate Rows:**
   - **Tool:** `remove_duplicates`
   - **Parameters:** 
     - `columns`: None (consider all columns).
     - `keep`: 'first' to retain the first occurrence.
     - `inplace`: True to modify the DataFrame directly.
   - **Expected Output:** Print the number of duplicate rows removed.

**Features Involved:**
- All features in both `train.csv` and `test.csv`.

**Constraints:** Ensure no data loss that might affect model training and testing.

**Example:**
```python
# Convert Data Types
convert_data_types(data=train, columns=['X_Minimum', 'X_Maximum', ...], target_type='float')
convert_data_types(data=test, columns=['X_Minimum', 'X_Maximum', ...], target_type='float')

# Remove Duplicates
remove_duplicates(data=train, inplace=True)
remove_duplicates(data=test, inplace=True)
```

### Task 4: Finalizing Cleaned Datasets

**Objective:** Create cleaned datasets and verify the final structure before proceeding to the next phase.

**Essential Actions:**
1. **Save Cleaned Datasets:**
   - **Tool:** Pandas (`data.to_csv()`)
   - **Parameters:** 
     - `path_or_buf`: File path for saving (`cleaned_train.csv` and `cleaned_test.csv`).
     - `index`: False to avoid saving index column.
   - **Expected Output:** Cleaned datasets saved as `cleaned_train.csv` and `cleaned_test.csv`.

2. **Verify Final Structure:**
   - Print summary statistics and data types to ensure all cleaning steps are correctly applied.

**Features Involved:**
- All features in both `train.csv` and `test.csv`.

**Constraints:** Ensure the cleaned datasets are ready for the next phase (In-depth EDA).

**Example:**
```python
# Save Cleaned Datasets
train.to_csv('cleaned_train.csv', index=False)
test.to_csv('cleaned_test.csv', index=False)

# Verify Final Structure
print(train.info())
print(train.describe())
print(test.info())
print(test.describe())
```

By following these detailed tasks, you can effectively clean your datasets, address missing values, treat outliers, ensure data consistency, and prepare the cleaned datasets for the next phase of your project.