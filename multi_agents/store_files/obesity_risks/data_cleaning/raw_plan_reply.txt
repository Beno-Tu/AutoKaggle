Great! With the provided information, let's proceed with a detailed plan for the Data Cleaning phase. We will break down the tasks into smaller, manageable steps focusing on addressing missing values, handling outliers, ensuring data consistency, and removing duplicates.

### Task 1: Address Missing Values
**Objective:** Identify and handle missing values in the dataset to ensure data completeness and integrity.
**Essential Actions:**
1. **Identify Missing Values:**
   - Use `data.isnull().sum()` to identify columns with missing values.
   - Print the number of missing values for each column.

2. **Handle Missing Values:**
   - For numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`):
     - Use the `fill_missing_values` tool with the method set to 'mean' for imputation.
   - For categorical features (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC`, `CALC`, `MTRANS`):
     - Use the `fill_missing_values` tool with the method set to 'mode' for imputation.

**Constraints:**
- Ensure that the imputation method is appropriate for the type of feature.
- Avoid using the 'constant' method unless there is a specific reason to fill with a fixed value.

**Tools:**
- `fill_missing_values`

**Expected Output:**
- A dataset with no missing values.

### Task 2: Handle Outliers
**Objective:** Detect and treat outliers to prevent them from skewing the analysis and model performance.
**Essential Actions:**
1. **Identify Outliers:**
   - Generate boxplots for numerical features to visualize outliers.
   - Use `detect_and_handle_outliers_iqr` with `factor=1.5` to identify outliers in numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`).

2. **Handle Outliers:**
   - For numerical features, use the `detect_and_handle_outliers_iqr` tool with the method set to 'clip' to cap outliers within the IQR range.

**Constraints:**
- Ensure that the IQR method is appropriate for the feature distribution.
- Avoid removing outliers entirely, as this may lead to loss of valuable data points.

**Tools:**
- `detect_and_handle_outliers_iqr`

**Expected Output:**
- A dataset with outliers capped to reduce their impact.

### Task 3: Ensure Data Consistency
**Objective:** Standardize categorical values and ensure consistent data types across the dataset.
**Essential Actions:**
1. **Standardize Categorical Values:**
   - Convert all categorical features to lowercase to ensure consistency (e.g., 'yes' vs. 'Yes').
   - Use the `convert_data_types` tool to ensure all categorical features are of type `str`.

2. **Convert Data Types:**
   - Use the `convert_data_types` tool to ensure numerical features (`Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`) are of type `float`.
   - Ensure the `id` column is of type `int`.

**Constraints:**
- Avoid changing the inherent meaning of the data during conversion.
- Ensure that all transformations maintain data integrity.

**Tools:**
- `convert_data_types`

**Expected Output:**
- A dataset with consistent and standardized data types and values.

### Task 4: Remove Duplicates
**Objective:** Identify and remove duplicate rows to ensure data uniqueness.
**Essential Actions:**
1. **Identify Duplicates:**
   - Use `data.duplicated()` to identify duplicate rows in the dataset.
   - Print the number of duplicate rows identified.

2. **Remove Duplicates:**
   - Use the `remove_duplicates` tool to remove duplicate rows from the dataset.
   - Set the `keep` parameter to 'first' to retain the first occurrence of each duplicate.

**Constraints:**
- Ensure that removing duplicates does not unintentionally delete important data.
- Verify the dataset after removing duplicates to ensure no unintended changes.

**Tools:**
- `remove_duplicates`

**Expected Output:**
- A dataset with unique rows, free of duplicates.

### Summary
By following these detailed steps, we aim to produce cleaned datasets (`cleaned_train.csv` and `cleaned_test.csv`) that are ready for further analysis and modeling. Here is a concise summary of the tasks:

1. **Address Missing Values**:
   - Identify and impute missing values for both numerical and categorical features.

2. **Handle Outliers**:
   - Identify outliers using the IQR method and cap them to reduce their impact.

3. **Ensure Data Consistency**:
   - Standardize categorical values and ensure consistent data types.

4. **Remove Duplicates**:
   - Identify and remove duplicate rows to ensure data uniqueness.

Let's proceed with implementing these tasks, ensuring that the cleaned datasets maintain data integrity and are ready for subsequent phases.